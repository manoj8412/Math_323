\documentclass{article}

\usepackage{Style_Thesis-Report}

\title{Probability}
\author{Ignacio Sada S\'{o}lomon}
\date{Winter 2020}  %Place'%' at front to activate today's date


\begin{document}
\clearpage\maketitle
\thispagestyle{empty}
\vspace{2cm}

\begin{abstract}
	Sample space, events, conditional probability, independence of events, Bayes' Theorem. Basic combinatorial probability, random variables, discrete and continuous univariate and multivariate distributions. Moment generating functions Independence of random variables. Chebyshevâ€™s inequality,central limit theorem, weak law of large numbers (if time allows).
\end{abstract}

\newpage

\tableofcontents
\newpage
\setcounter{page}{1}
\cfoot{\thepage}

\section{Fundamentals}
\subsection{Set Theory}

	\begin{defn}
		A \textbf{set} is a collection of objects, called elements.
	\end{defn}
	\begin{exmp}
		The set of natural numbers $\N = \{ 1, 2, 3, \dots\}$, which is both infinite and countable.
	\end{exmp}
	\begin{exmp}
		The set of all McGill students.
	\end{exmp}
	\begin{defn}
		Let $X$ be a set. A \textbf{subset} is another set $A$ such that every element of $A$ is in $X$ as well:
		$$ A = \{ x: x \in X\} \quad \text{ or } \quad x \in A \implies x \in X$$
	\end{defn}
	\begin{rem}
		The empty set is denoted $\varnothing = \{\}$
	\end{rem}
	\begin{defn}
		An \textbf{intersection} between the sets A and B is defined as
		$$ A \cap B = x \in A \quad \text{and}\quad x \in B$$
	\end{defn}
	\begin{defn}
		A \textbf{union} between the sets A and B is defined as
		$$ A \cup B = x \in A \quad \text{or}\quad x \in B$$
	\end{defn}
	\begin{defn}
		Let $X$ be a universal set with a subset $A$. Then the complement of $A$ with respect to $X$ is
		$$ A^c = X \setminus A = \{ x: x \in X \quad \text{and} \quad x \notin A \}$$
	\end{defn}
	\begin{exmp}
		$\mathcal{P} (B) = \{ A: A \subseteq B \}$ (all subsets of $B$)
		
		Let $B = \{1,2,3\}$, then
		$$ \mathcal{P}(B) = \bigg\{ \underbrace{ \{\}, \{1,2,3\}}_{\text{trivial subsets}}, \underbrace{\overbrace{\{1\}, \{2\}, \{3\},}^{\text{singletons}} \overbrace{\{1,2\}, \{1,3\}, \{2,3\}}^{\text{doubles}}}_{\text{proper subsets}} \bigg\}$$
	\end{exmp}
	\begin{defn}
		Two sets $A$ and $B$ are denoted as \textbf{disjoint} whenever they share no elements, i.e. they have nothing in common:
		$$ A \cap B = \varnothing$$ 
	\end{defn}
	\begin{exmp}
		Let $\Omega = \N = \{ 0,1,2,3,4,\dots\}$ and $A \subseteq N = \{ n \in \N: n \text{ is even}\}$. Then,
		$$ A^c = \{ n \in \N : n \text{ is odd}\}$$
	\end{exmp}
\pagebreak
Here are some important set properties:
	\begin{multicols}{2}
		\begin{itemize}
			\item $A \cap \varnothing = \varnothing$
			\item $A \cup \varnothing = A$
			\item $A \cup B = B \cup A$
			\item $A \cap B = B \cap A$
			\item $(A\cap B)\cap C = A \cap (B \cap C)$
			\item $(A \cup B) \cup C = A \cup (B \cup C)$
			\item $(A \cap B)^c = A^c \cup B^c$
			\item $(A \cup B)^c = A^c \cap B^c$
		\end{itemize}
	\end{multicols}
	\begin{exe}
		Prove that
		\begin{enumerate}[$\quad \quad$a)]
			\item $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$
			\item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
		\end{enumerate}
	\end{exe}
	\begin{note}
		Consider $A \setminus B = \{ x\in A: x \notin B \}$. Then we define the \textbf{exclusive or} as
		$$ A \setminus B \cup B \setminus A = A \triangle B$$
	\end{note}
\pagebreak
\section{Probability}
	\subsection{Introduction}
	Probability is the branch of applied mathematics that deals with random events, which are non-deterministic in nature, unlike calculus for instance, which deals with deterministic events.
	\begin{exmp}
		Toss a coin. We won't know the outcome of the toss, but we do know that the outcome will be either \emph{heads} or \emph{tails}.
	\end{exmp} 
	\begin{exmp}
		Roll a die. We now have several outcomes, depending on the number of faces on the die.
	\end{exmp}
	\begin{defn}
		Given a random experiment, the set of all possible outcomes is denoted $\Omega$, and is called the \textbf{sample space}.
	\end{defn}
	\begin{exmp} $\quad$ \\
		\vspace{-0.5cm}
		\begin{itemize}
			\item The sample space for a coin toss is $\Omega = \{ H, T \}$
			\item The sample space for a rolled 6-face die is $\Omega = \{1,2,3,4,5,6\}$
			\item The sample space for a coin toss where we require heads once is $\Omega = \{ H, TH, TTH, \dots \}$
		\end{itemize}
	\end{exmp}
	\begin{defn}
		Let $\Omega$ denote the set of all possible outcomes of a random experiment.
		\begin{enumerate}[$\quad\,\,${2.2}.1.]
			\item An \textbf{elementary event} is a subset of $\Omega$ with a singular element (i.e. one single outcome) and is also an element of $\mathcal{P}(\Omega)$.
			\item A \textbf{compound event} is any subset of $\Omega$ (including the elementary events).
			\item An \textbf{event} is any element of $\Omega$.
			\item $\varnothing$ denotes \textbf{impossible events}.
			\item A \textbf{complimentary event} given an event $A$ is denoted $A^c$.
			\item Given 2 events $A$ and $B$, where $A \cap B = \varnothing$, we denote them as \textbf{disjoint events}.
		\end{enumerate}
	\end{defn}
	\begin{defn}
		Let $\Omega$ be the sample space attached to a random experiment. A \textbf{probability} is a function $P$ such that
		$$ P: \mathcal{P}(\Omega) \to [0,1]$$
	\end{defn}
	Note that $P(\Omega)=1$. Given a sequence $A_1, \dots, A_n, \dots$, of pairwise disjoint events ($A_i \cap A_j = \varnothing \quad \forall i \neq j$), we have that 
	$$ P \left(  \bigcup_{i =1} A_i\right) = \sum_{i =1} P (A_i)$$
	\begin{rem}
		In general, if $\Omega$ is countable, then it is enough to define $P$ on the elementary event, as every other event can be written as a union.
	\end{rem}
	\begin{exmp}
		Toss a coin such that $\Omega = \{ H, T \}$. A probability on $\Omega$ is completely given by $p \in [0,1]$, and the assignment of $P(\{H\}) = p$, and $P(\{T\}) = 1-p$.
	\end{exmp}
	\begin{rem}
		If $p=1/2$ in the previous example, then $P(\{H\}) = P(\{T\}) = 1/2$, implying that \emph{the coin is fair/balanced}.
	\end{rem}
	\begin{exmp}
		Throw a die such that $\Omega = \{ \omega_1, \omega_2, \omega_3, \omega_4, \omega_5, \omega_6 \}$. A probability on $\Omega$ is given by 6 non-negative integers $p_i$ where $i=1,2,3,4,5,6$, such that
		$$ p_i = P(\{w_i\}) \geq 0$$
		And so
		$$ \sum_{i=1}^6 p_i = 1$$
		Therefore, if we consider the die to be fair, we have $p_i = 1/6 \quad \forall i \in \{ 1,2,3,4,5,6 \}$. \\ Let $A = \{ \omega_1, \omega_3\}$ Then
		\begin{align*}
		 A &= \{ \omega_1\} \cup \{ \omega_3 \} \implies P(A) \\ 
		 &= P(\{ \omega_1 \}) + P(\{ \omega_3 \}) \\
		 &= 1/6 + 1/6 \\
		 &= 1/3
		\end{align*} 
	\end{exmp}
	\begin{exe}
		Consider a die such that the probability $P(\{\omega_i\})$ is proportional to $k$ such that
		$$ P \big( \{\omega_i\} \big) = ck$$
		Therefore:
		\begin{align*}
			1 &= \sum_{k=1}^6 P(\{\omega_k\}) \\
			&= \sum_{k=1}^6 ck \\
			&= c\sum_{k=1}^6 k \\
			&= c\frac{n(n+1)}{2}\bigg|_{n=6} 
				\intertext{}
			&= c \frac{6 \times 7}{2} \\
			&= 21c\\
			\therefore c &= \frac{1}{21}
		\end{align*}
		
		What would be the probability of an even number?\\
		Let $A = \{ \omega_2, \omega_4, \omega_6 \}$, and so
		\begin{align*}
			P(A) &= P(\{\omega_2\}) + P(\{\omega_4\}) + P(\{\omega_6\})  \\
			&= \frac{2+4+6}{21} \\
			&= \frac{12}{21}
		\end{align*}
		
		What would be the probability of an odd number?\\
		Simple:
		\begin{align*}
			P(B) &= 1- P(A) \\
			&= \frac{9}{21}
		\end{align*}
	\end{exe}
	\begin{exmp}
		Toss a coin until heads appears. We then have:
		$$ \Omega = \{ \omega_1 = H, \omega_2 = TH, \omega_3 = TTH, \dots, \omega_n = \underbrace{TTT}_{n-1}H, \dots \}$$
		$$ \therefore P \big( \{ \omega_n \} \big) = c \left( \frac{1}{3}\right)^n$$
		We now let $1 = c \left( \sum_{n=1}^\infty \left( \frac13\right)^n \right)$ such that
		\begin{align*}
			c &= \frac{1}{\sum_{n=1}^\infty \left( \frac13 \right)^n} \\
			&= \frac{1}{\frac{1}{3} \left( \frac{1}{1-1/3} \right)} \\
			&= \frac{1}{\frac{1}{3} \left( \frac{3}{2} \right)} \\
			&= \frac{1}{\frac12}\\
			&= 2
		\end{align*}
\pagebreak

		Thus, $P \big( \{ \omega_n \} \big) = 2 \left(\frac{1}{3}\right)^n \implies H$ appears in an even number of trials. Then, what is $P(A)$? Recall that \\ $A = \{ \omega_1, \omega_2, \dots, \omega_n, \dots\} $ so \\
		\vspace{-1cm}
		\begin{align*}
			P(A) &= \sum_{n=1}^\infty P \left( \{ \omega_n \} \right) \\
			&= \sum_{n=1}^\infty 2 \left( \frac13 \right)^{2n} \\
			&= 2 \sum_{n=1}^\infty \left( \frac{1}{3^2} \right)^{2n} \\
			&= 2 \cdot \frac19 \left( \frac{1}{1-\frac{1}{9}}\right) \\
			&= \frac14
		\end{align*}
	\end{exmp}
	\begin{thm}
		Let $A, B$ be subsets of a set $\Omega$. The probability function $P: \Omega \to [0,1]$ has the following properties:
		\begin{enumerate}[$\quad\quad$1)]
			\item $P(\varnothing) = 0$ 
			\item $P(A^c) = 1 - P(A)$
			\item $P(A \cup B) =  P(A) + P(B) - P(A \cap B)$
		\end{enumerate}
	\end{thm}
	\begin{proof}
		\begin{enumerate}[$\quad\quad$1)]
			\item 
			\begin{align*}
				\Omega = \Omega \cup \varnothing &\iff \Omega \cap \varnothing = \varnothing \\
				&\iff P(\Omega) = P(\Omega \cup \varnothing)\\
				&\iff P(\Omega) + P(\Omega) \\
				&\iff 1= 1+ P(\varnothing) \\
				&\iff P(\varnothing) = 0
			\end{align*}
			\item 
			\begin{align*}
				\Omega = A \cup A^c &\iff A \cap A^c = \varnothing \\
				&\iff 1 = P(\Omega) = P(A) + P(A^c)\\
				&\iff P(A^c) = 1 - P(A)
			\end{align*}
			\item
			\begin{align*}
				A \cup B = (A \setminus B) \cup B &\iff (A \setminus B) \cap B = \varnothing \implies P(A \cup B) = P(A \setminus B) + P(B)\\
				\therefore A = (A\setminus B) \cup (A \cap B) &\iff (A \setminus B) \cap (A \cap B) = \varnothing \implies P(A) = P(A \setminus B) + P(A \cap B) \\
				&\iff P(A \setminus B) = P(A) - P(A \cap B) \\
				\therefore P(A \cup B) &= P(A) + P(B) - P(A \cap B)
			\end{align*}
		\end{enumerate}
	\end{proof}
	\begin{exe}
		Show that $P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) -  P(B \cap C) + P(A \cap B \cap C)$
	\end{exe}
	\begin{defn}
		Let $\Omega_1, \Omega_2$  be two finite sample spaces.
		\begin{enumerate}[$\quad\quad$1)]
			\item $\Omega_1 \times \Omega_2 = \{ (\omega_1, \omega_2): \,\,  \omega_1 \in \Omega_1, \,\,  \omega_2 \in \Omega_2 \}$
			\item $|\Omega_1 \times \Omega_2| = |\Omega_1| \cdot |\Omega_2|$
		\end{enumerate}
	\end{defn}
	\begin{exmp}
		Roll a die twice. What is the probability that the sum of the number obtained is 7? We will define the set of events where the sum of both rolls is 7 as $A$:
		\begin{align*}
			\Omega = \Omega_1 \times \Omega_2 \implies |\Omega| = |\Omega_1| \cdot |\Omega_2| = 36 \\
			\Omega_1 = \Omega_2 = \{ \omega_1, \omega_2, \omega_3, \omega_4, \omega_5, \omega_6 \}
		\end{align*}
		Thus,
		\begin{align*}
			A = \{ (\omega_1, \omega_6), (\omega_2, \omega_5), &(\omega_3, \omega_4), (\omega_4, \omega_3), (\omega_5, \omega_2), (\omega_6, \omega_1)  \} \\
			\therefore P(A) &= \frac{|A|}{|\Omega|} = \frac{6}{36} = \frac16
		\end{align*}
	\end{exmp}
	\begin{exe}
		Find the probability such that the sum of the number obtained in both rolls is even. 
		\\
		(\emph{Ans: 1/2})
	\end{exe}
	\begin{defn}
		A \textbf{permutation} of $r$ elements chosen from $n$ elements is equivalent to throwing successively without replacement $r$ elements from a urn which contains $n$ elements. The total number of combinations for a permutation is
		$$ C_r^n = \binom{n}{r} = \frac{P_r^n}{r!} = \frac{n!}{r! (n-r)!} $$
	\end{defn}
	Note that:
	$$ P_r^n = n(n-1)(n-2)\dots (n-r+1) = \frac{n!}{(n-r)!}$$
	is the number of possible combinations.
	\begin{exmp}
		A urn contains 4 balls; 1 green ball, 1 blue ball, 1 red ball, and 1 yellow ball. Draw successively without replacement 3 balls from the urn.
	\end{exmp}
	\begin{exmp}
		A hand is a subset of 5 cards from a deck of 52 cards.
		$$ C_r^n = C_5^{52} = \frac{52!}{5! (47!)} = \frac{48 \times 49 \times 50 \times 51 \times 52}{1 \times 2 \times 3 \times 4 \times 5}$$
		What is the probability that a selected (or random) hand will contain at least one Jack?
		Let $A = $``At least 1 Jack" such that  $A^c =$ ``No jack". Then,
		\begin{align*}
			P(A^c) &= \frac{C^{48}_5}{C^{52}_5} &P(A) = 1-P(A^c)
		\end{align*}
		If we let $A_i = $``At least $i$ Jacks" for $i=1,2,3,4$, then
		\begin{align*}
			P(A) &= P\left( \bigcup_{i=1}^4 A_i \right)  \\
			&= \sum_{i=1}^4 P(A_i) &P(A_i) = \frac{C_i^4 \times C_{5-i}^{48}}{C_{5}^{52}}
		\end{align*}
		\begin{rem}
			$C_r^n$ is the number of subsets with $r$ elements of a set $S$ which has $n$ elements.
		\end{rem}
		Consider this alternative solution:
		\begin{align*}
			P(A) &= 1 - \overbrace{P(A^c)}^{\text{No jack}} \\
			\therefore P(A^c) &= \frac{C_{5}^{48}}{C_5^{52}} \implies P(A) = 1- \frac{C_{5}^{48}}{C_5^{52}} 
		\end{align*}
	\end{exmp}
	\begin{exe}
		The letters of the word ``ANANAS" are written on 6 marbles. Select 3 marbles successively, without replacement, from the initial 6 marbles to form a 3 letter word.
	\end{exe}
\hfill
	\begin{thm}
		\textbf{The Binomial Theorem:}
		$$ (a+b)^n = \sum_{k=0}^n C_k^n a^{k}b^{n-k} $$
	\end{thm}
	\begin{exmp}
		$(a+b)^4 = b^4 + 4ab^3 + 6a^2b^2 + 4a^3 b + a^4$
	\end{exmp}
	\begin{exmp}
		Find the coefficient of $x^7$ in the expansion of $(2+3x^2)^6$ \\
		(\emph{Ans: $C_{4}^5 3^4 2^3$})
	\end{exmp}
\pagebreak

	We can apply this concept to find the \emph{power set of a fine set}. Let $\Omega$ be a finite set. $\mathcal{P}(\Omega)$ is the power set of $\Omega$. If $|\Omega| = n$, then $|\mathcal{P}(\Omega)|$ is
	\begin{align*}
		|\mathcal{P}(\Omega)| &= C_0^n + C_1^n + C_2^n + \dots + C_k^n + \dots + C_n^n \\
		&= \sum_{k=0}^n C_k^n \\
		&= \sum_{k=0}^n (1)^k (1)^{n-k} C_k^n \\
		&= (1+1)^n \\
		&\boxed{= 2^n}
 	\end{align*}
	
	\begin{exmp}
		\begin{align*}
			(a+b)^4 &= \sum_{k=0}^4 C_k^4 a^k b^{4-k} \\
			&= b^4 + 4ab^3 + 6a^2 b^2 + 4a^3 b + a^4
		\end{align*}
	\end{exmp}
\hfill
\subsection{Conditional Probability}
	Consider rolling a fair die. The probability that we obtain 2 as a result is $\frac{1}{6}$. However, the probability of obtaining 2 from only even numbers is $\frac{1}{3}$.
	
	\begin{defn}
		Let $\Omega$ be a parent set of events, and let us consider subsets of events $A \subset \Omega$ and $B \subset \Omega$.  Then, the \textbf{conditional probability} of $A$ given $B$ is
		$$ P(A \mid B ) = \frac{P (A \cap B)}{P(B)}$$
	\end{defn}
	\begin{center}
		\begin{tikzpicture}
		\draw \firstcircle node[below]{$A$};
		\draw \secondcircle node[below]{$B$};
		\draw (-2cm, -2cm) -- (4cm, -2cm) -- (4cm, 2cm) -- (-2cm, 2cm) -- (-2cm, -2cm) node[above=4mm, right=2mm]{$\Omega$};
		\begin{scope}
			\clip \firstcircle;
			\fill[grey!40] \secondcircle;
		\end{scope}
		\end{tikzpicture}
	\end{center}
	\begin{rem}
		The map $A \mapsto P(A \mid B)$ is a probability on $\mathcal{P}(\Omega)$.
	\end{rem}
\pagebreak
	\begin{exmp}
		Throw a fair die. We define the following events:
		\begin{multicols}{3}
			\begin{itemize}
				\item $A= $ result is even.
				\item $B= $ either 1, 2, or 5
				\item $C= $ either 1 or 2
			\end{itemize}
		\end{multicols}
		Thus:
		\begin{align*}
		P(A)  = \frac12 && P(B) = \frac12 && P(C) = \frac13 &&  \\
		 P(A \cap C) = \frac16 && P(B \cap C) = \frac13 && P (A \cap B) = \frac16 && \\
		\intertext{Hence:}
		P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac13 && P(A \mid C) = \frac{P(A \cap C)}{P(C)} = \frac12  &&P(B\mid A) = \frac{P(A \cap B)}{P(A)} = \frac13 && \\
		 P(C \mid A) = \frac{P(A \cap C)}{P(A)} = \frac13 && P(B \mid C) = \frac{P(B \cap C)}{P(C)} = 1 && P(C\mid B) = \frac{P(B \cap C)}{P(B)} = \frac23 &&
		\end{align*}
	\end{exmp}

	\begin{defn}
		Two events $A$ and $B$ are said to be \textbf{independent} if at least one of the following statements is true:
			\begin{multicols}{3}
			\begin{itemize}
				\item $P(B \mid A) = P(B)$ 
				\item $P(A \mid B) =P(A)$
				\item $P(A \cap B) = P(A ) P(B)$
			\end{itemize}
		\end{multicols}
	\end{defn}
	\begin{exmp}
		In Example 2.13:    
		\begin{itemize}
			\item$A$ and $B$ are not independent $\left(P(A \cap B) = \frac16, \quad P(A)P(B) = \frac14\right)$
			\item $A$ and $C$ are independent $\left(P(A \cap C) = \frac16, \quad P(A)P(C) = \frac16\right)$
			\item $B$ and $C$ are not independent $\left(P(B\cap C) = \frac13, \quad P(B)P(C) = \frac16\right)$
		\end{itemize}
	\end{exmp}
	\begin{exmp}
		Toss a coin (which is such that $P(H) = p \in (0,1)$)twice. What is the probability that we will obtain $T$ twice? 
		$$ P (\underbrace{\{ T\} \times \{T\} }_{TT} )$$ 
		By design, the outcome obtained on the first toss is \emph{independent from the outcome obtained in the second toss:}
		\begin{align*}
			P(TT) &= P \big( \{T\} \big) \times  P \big( \{T\} \big) \\
			&= (1-p)^2
		\end{align*}
	\end{exmp}
	\begin{exmp}
		Toss a coin such that $P \big(  \{H\} \big) = p \in (0,1)$ until $H$ appears. 
		\begin{sol}
			We have:
			$$ \Omega = \{ H, TH, TTH, TTTH, \,\, \dots\, \, , \underbrace{T\dots T}_{n-1}H \}$$
			\begin{align*}
			\therefore P(\omega_n) &= \big( P(T) \big)^{n-1} \times \big( P(H) \big) \\
			&= (1-p)^{n-1} \times (p)
			\intertext{We now recall infinite series expansion to evaluate $P(\omega_n)$:}
			\sum_{n=1}^\infty (1-p)^{n-1} (p) &= p \sum_{n=1}^\infty (1-o)^{n-1} \\
			&= p \sum_{n=0}^\infty (1-p)^n \\
			&= p \left( \frac{1}{1-(1-p)} \right) \\
			&= 1
			\end{align*}
		\end{sol}
 	\end{exmp}
 \begin{thm}
 	If $A$ and $B$ are independent ($A \indp B$), then so are $A^C$ and $B^C$. Consequently:
 	\begin{align*}
 		A^C \indp B && B^C \indp A
 	\end{align*}
 \end{thm}
	\begin{proof}
		\begin{align*}
			P(A^C \cap B ) &= P (B \setminus A) \\
			&= P(B) - P(A \cap B) \\
			&= P(B) -P(A)P(B) \\
			&= P(B) - (1- P(A)) \\
			&= P(B) P(A^C)
		\end{align*}
		$B^C$ and $A$ are similarly shown to be independent.
	\end{proof}
	\hfill
	\begin{exmp}
		Let $P(A) = 0.7$, $P(B)=0.8$, $A$ and $B$ are independent ($A \indp B$). Find $P(A^C \cap B^C)$.
	\pagebreak
		\begin{sol}
			$ $
			\begin{align*}
				P(A^C\cap B^C) &= P(A^C)P(B^C)\\
				&= (0.3)(0.2)\\
				&\boxed{= 0.06}
			\end{align*}
		\end{sol}
	\end{exmp}
	\begin{exmp}
		Let $P(A) = 0.7$, $P(B)=0.8$, $A$ and $B$ are independent ($A\indp B$). Find $P(A \cup B)$.
		\begin{sol}
			\begin{align*}
				(A \cup B)^C &= A^C \cap B^C \\
				&= P(A) + P(B) - P(A \cap B) \\
				&= P(A) + P(B) - P(A)P(B)\\
				&= 0.7 + 0.8 - 0.56 \\
				&\boxed{=0.94}
			\end{align*}
		\end{sol}
	\end{exmp}
 	\begin{exe}
 		A die is such that $P\big( \{ \omega_2 \} \big) = p$, $P\big( \{\omega_4\} \big) = q$, and $p+q \in (0, 1)$. Roll the die until 2 or 4 appears. What is the probability that 2 appears first?\\
 		(\emph{Ans: $\frac{p}{p+q}$} )
 	\end{exe}
\subsection{Baye's Rule}
	\begin{exmp}
		On a given exam, we are given the following distribution:
		\begin{table}[h]
			\begin{tabular}{c|c|c|c}
				& Success (S) & Failure (Fl) & Total \\ \hline
				Male (M)   & 0.3         & 0.1         & 0.4   \\ \hline
				Female (Fm) & 0.4         & 0.2         & 0.6   \\ \hline
				Total      & 0.7         & 0.3         & 1    
			\end{tabular}
		\end{table}
		Here,
		\begin{center}
			\begin{multicols}{2} 
				\begin{itemize}
					\item $P(M) = 0.4$
					\item $P(Fm) = 0.6$
					\item $P(S \mid M) = \frac34$
					\item $P(S \cap M)=0.3$
					\item $P(S \cap Fm)=0.4$
					\item $P(S \mid Fm) = \frac23$
				\end{itemize}
			\end{multicols}
		\end{center}
		Thus, 
		\begin{align*}
			P(S) &= P(M \cap S) + P(Fm\cap S)  \\
			&= P (S \mid M) P(M) + P (S \mid Fm) P(Fm) \\
			&= \left( \frac34 \times \frac{4}{10} \right) + \left( \frac23 \times \frac{6}{10} \right)\\
			&\boxed{= \frac{7}{10}} 
		\end{align*}
	\end{exmp}
	\begin{defn}
		Let $\Omega$ be a sample space. A \textbf{finite partition } of $\Omega$ is given by events $A_1, A_2, A_3, \,\, \dots, A_n$, which are pairwise disjoint, and also 
		$$ \bigcup_{n \in \N^*} A_n= \Omega $$
	\end{defn}
	\begin{figure}[h]
		\begin{tikzpicture}
			\draw (0,0) -- (4, 0) -- (4, 2) -- (0,2) -- (0,0) node[right= 5mm, above=7mm]{$A_1$};
			\draw (1, 0) -- (1, 2) node[right= 5mm, below=7mm]{$A_2$} ;
			\draw (2, 0) -- (2, 2) node[right= 5mm, below=8mm]{$\dots$} ;
			\draw (3, 0) -- (3, 2) node[right= 5mm, below=7mm]{$A_n$};
		\end{tikzpicture}
	\end{figure}
	
	\begin{thm} \textbf{Baye's Theorem}\\
		Given a partition $A_1, A_2, \,\dots, A_n$ of $\Omega$, suppose that $P(A_1)$, $P(A_2)$, $\dots$, $P(A_n)$ are known. Suppose that for an event $B$, the following are known: $P(B \mid A_1), P(B \mid A_2), \dots, P(B \mid A_n)$. Thus:
		\begin{enumerate}
			\item $\quad$ 
			\begin{center}
				$P(B) = \sum_{i= 1}^n P(B \mid A_i) P(A_i) $
			\end{center}
			
			\item Let $k \in \{1, 2, \dots, n\}$ be fixed. Then: $$ P(A_k \mid B ) = \frac{P (B \mid A_k) P(A_k)}{\sum_{i=1}^n P(B\mid A_i) P(A_i)}$$
		\end{enumerate}
	\end{thm}
	\begin{proof}
		Let $B = \Omega$, then:

		\begin{align*}
			B &= B \bigcap \bigg( \bigcup_{n \in N\setminus \{0\}} A_n \bigg) \\
			&= \bigcup_{n \in N\setminus \{0\}} \bigg( B \bigcap A_n \bigg) \\
			\therefore P(B) &= \sum_{k=1}^n P(B \bigcap A_k) 
			\intertext{}
			\therefore P(A_i \mid B) &= \frac{P(B \bigcap A_i)}{P(B)} \\
			&= \frac{P(B \mid A_i) P(A_i)}{P(B)}
		\end{align*}
	\end{proof}
	\begin{exmp}
		We have 2 urns. Urn 1 has 3 red balls and 2 blue balls, and urn 2 has 5 red balls and 6 blue balls. We now select a ball from urn 1, place it in urn 2, and select a ball from urn 2. What is the probability that said ball is red?
		\begin{sol}
			Let $A$ be the event where the ball from urn 2 is red, and $B$ where the ball from urn 1 is blue. By Baye's Theorem,
			\begin{enumerate}
				\item$ P(A) = \frac{2}{5} \cdot \frac{5}{12} + \frac{1}{2}\cdot \frac{3}{9} = \frac{7}{15}$
				\item $P(B \mid A) = \frac{5}{12} $
			\end{enumerate}
		\end{sol}
	\end{exmp}
	\begin{exmp}
		On a weather forecast model we have that:
		\begin{enumerate}[$\quad\quad$a)]
			\item If today is rainy, then tomorrow is rainy with a probability of 0.6.
			\item If today is sunny, then tomorrow is sunny with a probability of 0.7.
		\end{enumerate}
		Suppose Monday is rainy. What is the probability that Wednesday is rainy?
		\begin{sol}
			Consider the following tree structure representing the probabilities of the forecast model:
			\begin{figure}[h]
				\begin{tikzpicture}
					[sibling distance = 4cm]
					\node [root] {Rainy}
						child { 
							node [env] {Rainy} 
								[sibling distance = 2cm]
								child {
									node [env] {Rainy}
									edge from parent node [above=2mm, left] {0.6}
								}
								child {
									node [env] {Sunny}
									edge from parent node [above=2mm, right] {0.4}
								}
							edge from parent node [above=2mm, left] {0.6}
						}
						child { 
							node [env] {Sunny} 
								[sibling distance = 2cm]
								child {
									node [env] {Rainy}
									edge from parent node [above=2mm, left] {0.3}
								}
								child {
									node [env] {Sunny}
									edge from parent node [above=2mm, right] {0.7}
								}
							edge from parent node [above=2mm, right] {0.4}
						};
				\end{tikzpicture}
			\end{figure}
		
			\noindent
			Let $B = $``Wednesday is rainy",  $A_1 = $ ``Tuesday is rainy", and $A_2=$ ``Tuesday is sunny." Then,
			\begin{align*}
				P(A_1) = 0.6 &\implies P(B \mid A_1) = 0.6 \\
				P(A_2) = 0.4 &\implies P(B \mid A_2) = 0.3\\
				\therefore P(B) &= P(B \mid A_1) P(A_1) + P(B \mid A_2) P(A_2) \\
				&= (0.6)(0.6) + (0.3)(0.4) \\
				&\boxed{= 0.48}
			\end{align*}
		\end{sol}
	\end{exmp}

\subsection{Multinomial Coefficients}
	We begin by recalling that \emph{binomial coefficients} are such that a combination with $C_k^n$ is clearly defined
	
	\begin{figure}[h]
		\begin{subfigure}{0.46\textwidth}
			\begin{center}
				\begin{tikzpicture}
					[sibling distance = 3cm]
					\node [root] {$n$}  
						child{
							node [env] {$k$}
							edge from parent node [above, left=2mm]{$S_1$}
						}
						child{
							node [env] {$k-1$}
							edge from parent node [above, right=2mm]{$S_2$}
						}
					;
					\draw node [above=4mm, right=2mm]{$S$};
				\end{tikzpicture}
			\end{center}
			\caption*{Partition of a set into two subsets.}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.46\textwidth}
			\begin{center}
				\begin{tikzpicture}
				[sibling distance = 3cm]
				\node [root] {$n$}  
				child{
					node [env] {$n_1$}
					edge from parent node [above, left=4mm]{$S_1$}
				}
				child{
					node [env] {$n_2$}
					edge from parent node [above, right=1mm]{$S_2$}
				}
				child{
					node [env] {$n_3$}
					edge from parent node [above, right=4mm]{$S_3$}
				}
				;
				\draw node [above=4mm, right=2mm]{$S$};
				\end{tikzpicture}
			\end{center}
			\caption*{Partition of a set into three subsets.}
		\end{subfigure}
	\end{figure}
	We consider the above cases to generalize by induction. In the case where we have 3 partitions, we have that  $n=n_1 = n_2 = n_3$, and so the total number of partitions is
	\begin{align*}
		C_{n_1}^n &= C_{n_3}^{n- n_1} \\
		&= \frac{n!}{(n-n_3)! n!} \\
		&= \frac{n!}{n_2 ! (n-n_1 -n_3)!} \\
		&= \frac{n!}{n_1 ! n_2 ! n_3 !}
	\end{align*}
	\begin{figure}[h]
		\begin{tikzpicture}
		[sibling distance = 3cm]
		\node [root] {$n$}  
		child{
			node [env] {$n_1$}
			edge from parent node [above, left=4mm]{$S_1$}
		}
		child{
			node [env] {$n_2$}
			edge from parent node [above, right=1mm]{$S_2$}
		} 
		child{ 
			node [env] {$n_k$}
			edge from parent node [above, right=4mm]{$S_k$}
		}
		;
		\draw node [above=4mm, right=2mm]{$S$}
		node[below=1.5cm, right=1.2cm]{$\dots$};
		\end{tikzpicture}
	\end{figure}

	Now suppose that we partition a set $S$ with $n$ elements into subsets $S_1, S_2, \dots, S_k$ with respective $n_1, n_2, \dots, n_k$ elements ($\sum_i n_i = n$), so the total number of partitions is given by 
	$$
		\boxed{
		\label{eq:partno}
		\binom{n}{n, n_1, \dots, n_k} = \frac{n}{n_1 ! n_2 ! \dots n_k !}
		}
	$$
	\begin{exmp}
		10 employees are split in $k=3$ ways for $3$ different tasks. There are 3 people assigned to task 1, 2 people assigned to task 2, and 5 people assigned to task 3. Suppose that the assignment is done randomly. What is the probability that Peter and John do not belong to the same team?
	\end{exmp}
	\begin{sol}
		Recall that 
		$$ C_{n_1}^n = C_{n_{2}}^{n-n_1} = C_{n_3}^{n-n_1 - n_2}$$
		Hence:
		\begin{align*}
			C_{n_1}^n &= \frac{n!}{n_1 ! \cancel{(n-n_1)!}} \cdot \frac{\cancel{(n-n_1)!}}{n_2 ! \cancel{(n-n_1 - n_2)!}} \cdot \frac{\cancel{(n-n_1 -n_2)!}}{n_3!} \\
			&= \frac{n!}{n_1 ! n_2 ! n_3 !} \\
			&= \binom{n}{n_1, n_2, n_3}
		\end{align*}
	Generally, if there are $k$ tasks and $n$ employees, with $n_1$ employees going to task $i$, then:
	$$ \sum_{i=1}^k n_i = n$$ \noindent We thus consider the cases where both Peter and John are on the same team:
	\begin{figure}[h]
		\begin{subfigure}{0.3\textwidth}
			\begin{center}
				$A_1^C$ \\ $\quad$
				
				\begin{tikzpicture}
				[sibling distance = 2cm]
				\node [root] {8}  
				child{
					node [env] {1}
				}
				child{
					node [env] {2}
				} 
				child{ 
					node [env] {5}
				}
				;
				\end{tikzpicture}
			\end{center}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.3\textwidth}
			\begin{center}
				$A_2^C$ \\ $\quad$
				
				\begin{tikzpicture}
				[sibling distance = 2cm]
				\node [root] {8}  
				child{
					node [env] {3}
				}
				child{
					node [env] {0}
				} 
				child{ 
					node [env] {5}
				}
				;
				\end{tikzpicture}
			\end{center}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.3\textwidth}
			\begin{center}
				$A_3^C$ \\ $\quad$ 
				
				\begin{tikzpicture}
				[sibling distance = 2cm]
				\node [root] {8}  
				child{
					node [env] {3}
				}
				child{
					node [env] {2}
				} 
				child{ 
					node [env] {3}
				}
				;
				\end{tikzpicture}
			\end{center}
		\end{subfigure}
	\end{figure}

\noindent
	Hence, $$P(A^C) = \frac{A_1^C + A_2^C + A_3^C}{\left( \frac{10!}{3! 2! 5!} \right)} $$ $$ \therefore P(A) = 1 - P(A^C) = 1 - \frac{A_1^C + A_2^C + A_3^C}{\left( \frac{10!}{3! 2! 5!} \right)}$$
	\end{sol}

	\begin{thm} \textbf{Multinomial Theorem}
		$$ (a_1 + a_2 + \dots + a_k)^n = \sum_{n_1 + n_2 + \dots + n_k = n} \binom{n}{n_1, n_2, \dots, n_k} \prod_{i=1}^k a_i^{n_i}$$
		where $ \binom{n}{n_1, n_2, \dots, n_k} = \frac{n!}{n_1 ! n_2 ! \dots n_k !}$
	\end{thm}
	\begin{exe}
		Consider $(1+k + 2k^5)^7$. Find the coefficient of $k^5$.
	\end{exe}
	\begin{exmp}
		$$ (a+b+c)^3 \implies \begin{array}{ccc}
		\boxed{(3,0,0)}& (1,2,0)&\boxed{ (0,2,1)} \\
		(2,1,0) & (1,0,2) &(0,0,3) \\
		(2,0,1) & (1,1,1) & (0,3,0)
		\end{array}$$
	\end{exmp}
\pagebreak
\section{Discrete Random Variables}
	We let $\Omega$ be a sample space. We assign a number to each probable element such that the sum of all elements is one, so for $k$ elements with corresponding probabilities $p_i$ for $i=1,2,\dots, k$, we have 
	$$ \sum_{i = 0}^k p_i = 1$$
	\begin{defn}
		A \textbf{random variable} $(r, \sigma)$ is a function $X: \Sigma \to \R$ such that:
		$$ \forall \omega \in \Omega: \quad X(\omega) \in \R$$
	\end{defn}	
	\begin{exmp}
		Flip a coin, $\Omega = \{ T, H\}$. Define $X(T) = -5$, $X(H)=10$. 
	\end{exmp}
	\begin{exmp}
		Toss a die and add the numbers obtained. We have:
		$$ \Omega = \left\{ (\omega_i, \omega_j):  
		\begin{array}{c}
			i = 1,2,3,4,5,6\\
			j=1,2,3,4,5,6
		\end{array}
		\right\}$$
		$$ \therefore X: \Omega \to \R \implies X(\omega_1, \omega_2) = i +j$$
	\end{exmp}
	\begin{defn}
		A \textbf{discrete random variable} is a random variable $X: \Omega \to \R$ that is said to be discrete if its \emph{range} $X(\Omega)$ is countable. Note that it can be either finite or infinite.
		\begin{figure}[h]
			\begin{tikzpicture} 
				% Omega Ellipse
				\draw (0,0) ellipse (1cm and 2.5cm) 
					node[above=2.2cm, left=1cm]{$\Omega$}
					(0,1.5) node{$p_1 \omega_1$} 
					(0,0.75) node{$p_2 \omega_2$}  
					(0,0) node{$p_3 \omega_3$}  
					(0, -0.75) node{$\vdots$} 
					(0,-1.5) node{$p_n \omega_n$}
				;
				% Real Line
				\draw (4, 2) -- (3, -2) 
					node[above=4.2cm, right = 1.2cm]{$\R$} 
					(3.8, 1.2) node{$\bullet$} (4.7, 1.2) node{$p_1+ p_2$}
					(3.5, 0) node{$\bullet$} (4, 0) node{$p_3$}
					(3.125, -1.5) node{$\bullet$} (3.7, -1.5) node{$p_n$}
				;
				% Arrows
				\draw [->,  >=stealth, auto ] (0.5,1.5) to [out=20, in = 160] (3.75, 1.2) ;
				\draw [->,  >=stealth, auto ] (0.5,0.75) to [out=-20, in = -160] (3.75, 1.2) ;
				\draw [->,  >=stealth, auto ] (0.5, 0) to [out=-10, in = -170](3.45, 0);
				\draw [->,  >=stealth, auto ] (0.5, -1.5) to [out=-10, in = -170] (3.08, -1.5);
			\end{tikzpicture}
		\end{figure}
	\end{defn}
	\begin{defn}
		Let $X$ be a discrete random variable on a sample space $\Omega$, and let $P$ be the probability on $\mathcal{P}(\Omega)$. The \textbf{probability function} of $X$ is the function denoted $P_X: \R \to [0,1]$ such that
		\vfill
		$$ P_X(X) = \underline{P}(X=t)$$
		\pagebreak
		\begin{rem}
			$\{X=t\}$ is an event such that $X^{-1} (\{ t\})$. For a discrete random variable, if $t \notin X(\Omega)$, then $\{X=t\} = \varnothing$, meaning that $P_X(X)=0$. If $t \in X(\Omega)$, then we have that
			$$ P_X(t) = \sum_{X(\Omega) = t} P(\{\omega\})$$
		\end{rem}
	\end{defn}
	\begin{exmp}
		Let the sample space of a coin toss be $\Omega = \{H, T\}$. For a fair coin, we earn 10 credits for heads but lose 5 credits for tails. Find the probability. What is the probability of losing 5 credits?
		\begin{sol}
			We have the following probability table:
			\begin{table}[h]
				\begin{tabular}{c|c|c|}
					Credits  & -5        & 10        \\ \hline
					$P_X(t)$ & $\frac12$ & $\frac12$
				\end{tabular}
			\end{table}
		
		\noindent
		Therefore, 
		\begin{align*}
			\left. \begin{array}{c}
			X(T) =-5 \\
			X(H) = 10
			\end{array} \right\} P_X(-5) &= P(X=-5) \\
			&= P(T) \\
			&= \frac12
		\end{align*}
		\end{sol}
	\end{exmp}
	\begin{exmp}
		Suppose that an item selected at random from a production line can have up to two types of defects:
		\begin{align*}
			D_1 &= \text{``Item has defect of type 1."} \\
			D_2 &= \text{``Item has defect of type 2."}
		\end{align*}
		Then, we assign $P(D_1) = 0.7$, $P(D_2) = 0.5$, and $P(D_1 \cap D_2) = 0.3$. Let $X$ be the possible number of defects that an item selected at random can have such that:
		$X(\Omega) = \{0,1,2 \}$. This is the support for $P_X$. Hence,
		
		\begin{align*}
			P_X(2) &= P(X=2) \\
			&= P(D_1 \cap D_2) \\
			&= 0.3
		\end{align*}
		\begin{align*}
			P_X(1) &= P(X=1) \\
			&= P(D_1 \triangle D_2) \\
			&= P \big( (D_1 \cap D_2^C ) \cup (D_1^C \cap D_2)\big) \\
			&= P(D_1 \cap D_2^C) + P(D_1 \cap D_2^C)\\
			&= P(D_1) - P(D_1 \cap D_2) + P(D_2) - P(D_1 \cap D_2) \\
			&= 0.6
		\end{align*}
		\begin{align*}
			P_X(0) &= P(D_1^C \cap D_2^C) \\
			&= P\big( (D_1 \cup D_2)^C \big) \\
			&= 1 - P(D_1 \cup D_2)\\
			&= 1 - 0.9\\
			&= \frac{1}{10}
		\end{align*}
	\end{exmp}
\subsection{Cumulative Distribution Function}
\begin{defn}
	Let $X$ be a random variable (discrete or not). The \textbf{cumulative distribution function} of $X$ is the function:
	$$ F_X = \begin{cases}
	R \to [0,1] \\
	x \mapsto P \big( \{ t_1 \leq t_2 \} \big)
	\end{cases}$$
	It has the following properties:
	\begin{multicols}{2}
		\begin{enumerate}[(1)]
			\item $$\lim\limits_{t \to - \infty} F_X (t) = 0$$ $$\lim\limits_{t \to + \infty} F_X (t) = 1$$ 
			\item $F_X$ is \emph{nondecreasing}, meaning $$t_1 \leq t_2 \implies F_X(t_1) \leq F_X(t_2)$$
			\item $F_X$ is always right continuous, meaning $$\lim_{t \to t_0^+} F_X(t) = F_X(t_0) $$
			\item For a discrete random variable, $F_X$ is a \emph{step-function}.
		\end{enumerate}
	\end{multicols}
\end{defn}
\pagebreak
\begin{exmp}
	Given the following probability table, find $F_X$.
	\begin{table}[h]
		\begin{tabular}{c|c|c|c}
			t  & 0        & 1 & 2        \\ \hline
			$P_X(t)$ & 0.1 & 0.6 & 0.3
		\end{tabular}
	\end{table}
\end{exmp}
\begin{sol}
	We have the following range:
	\begin{center}
		\begin{tikzpicture}
			\draw (0, 0) -- (10, 0)
			;
			\foreach \x in {0, ..., 10}
				\draw (\x, -0.1) -- (\x, 0.1);
			\draw 
				(0,0) node[below=-1.6mm]{$\bullet$} node[above=2mm]{0.1} node[below=2mm]{0}
				(5,0) node[below=-1.6mm]{$\bullet$} node[above=2mm]{0.6} node[below=2mm]{1}
				(10,0) node[below=-1.6mm]{$\bullet$} node[above=2mm]{0.3} node[below=2mm]{2}
			;
		\end{tikzpicture}
	\end{center}
	\begin{figure}[h]
		\begin{subfigure}{0.49\textwidth}
			$$ 
			F_X = \begin{cases}
			0 & t \in (-\infty, 0) \\
			0.1 & t \in [0, 1) \\
			0.7 & t \in [1, 2) \\
			1 & t \in [2, + \infty)
			\end{cases}
			$$
		\end{subfigure}
	\hfill
		\begin{subfigure}{0.49\textwidth}
			\begin{center}
				\begin{tikzpicture}
					% Axes
					\draw [->, >=stealth, auto] (0,0) to (2.5, 0) node[right]{$t$} ;
					\draw [->, >=stealth, auto] (0,0) to (0, 3) node[above]{$P_X(t)$};
					\foreach \x in {1, 2}
						\draw (\x, -0.1) -- (\x, 0.1) node[below=2mm]{\x};
					\foreach \x in {0.2, 1.4, 2}
						\draw (-0.1, \x) -- (0.1, \x);
					\draw (0, 0.2) node[left=2mm]{0.1} (0, 1.4) node[left=2mm]{0.7} (0, 2) node[left=2mm]{1};
					% Data
					\draw 
						(0,0.2) node[below=-1.8mm]{$\bullet$} (1, 0.2) node[below=-1.8mm]{$\circ$}
						(1,1.4) node[below=-1.8mm]{$\bullet$} (2, 1.4) node[below=-1.8mm]{$\circ$}
						(2,2) node[below=-1.8mm]{$\bullet$} (0, 0) node[below=-1.8mm]{$\circ$}
					;
					\draw (0,0.2) -- (0.95,0.2);
					\draw (1,1.4) -- (1.95, 1.4);
				\end{tikzpicture}
			\end{center}
		\end{subfigure}
	\end{figure}
	\begin{rem}
		For a discrete random variable, we have that $$ P_X (t) = 0 \iff F_X \text{ is continuous at }t $$ $$F_X (t) \text{ is discontinuous at } t_0 \iff P_X (t_0) = F_X (t_0) - \lim_{t \to t_0 -} F_X (t) $$
	\end{rem}
\end{sol}
	We will now list properties for the probability function. Let $X$ be a random variable, and $P_X$ its probability function.
	\begin{enumerate}[$\quad\quad$(1)] 
		\item $P_X (t) \in [0,1] \quad \forall x \in \R$ 
		\item $\{ x \in \R: P(t) \neq 0 \}$ is discrete.
		\item $\sum_t P(t) =1$  (this is the sum for $x \in \{ t \in \R: P_X (t) \neq 0 \}$, which can be finite or an infinite series)
	\end{enumerate}
	\begin{prop}
		Let $P : \R \to [0,1]$ be a function such that
		\begin{enumerate}[$\quad\quad$(1)]
			\item $\{ t \in \R: P(t) \neq 0 \}$ is discrete.
			\item $\sum_t P(t) = 1 $
		\end{enumerate}
		Then there exists a discrete random variable $X$ such that $P$ is the probability function of $X$:
		$$ P=P_X$$
	\end{prop}
	\begin{exmp}
		Let $P$ be a function such that
		$$ P(t) = 
		\begin{cases}
		0 & t \leq 0 \in \Z \\
		\frac{k}{4t^2 + 2t} & t\in \N^* = \N \setminus \{0\}
		\end{cases}
		$$
		Find $k$ such that the function $P$ is the probability function of some random variable $X$.
	\end{exmp}
	\begin{sol}
		It is enough to find $k > 0$ such that 
		$$ \sum_{n=1}^{+\infty} \frac{k}{4n^2 + 2n} = 1$$
		Thus:
		\begin{align*}
			k&= \frac{1}{\sum_{n=1}^{+\infty} 4n^2 + 2n}  \\
			&= \sum_{n=1}^{+\infty} \left( \frac{1}{2n} - \frac{1}{2n+1}\right) \\
			&= \frac{1}{2} - \frac{1}{3} + \frac{1}{4} - \frac{1}{5}
		\end{align*}
	\end{sol}

\subsection{Expected Value}
	\begin{defn}
		Let $X$ be a random variable, and $P_X$ be its probability function. The \textbf{expected value} of $X$ (if it exists) is defined as:
		$$ \E(X)=\sum_t tP_X (t)$$
	\end{defn}
	\begin{rem}
		The expected value of $X$ is well-defined if $ \sum_t |t| P_X(t)$ is a convergent infinite series or is finite.
	\end{rem}
	\begin{exmp}
		Consider the following probability table:
		\begin{table}[h]
			\begin{tabular}{c|c|c|c}
				t & 0        & 1 & 2        \\ \hline
				$P_X(t)$ & 0.1 & 0.6 & 0.3
			\end{tabular}
		\end{table}
		Then the expected value $\E(X)$ is:
		\begin{align*}
			\E(X) &= \sum_t tP_X(t) \\
			&= 0 \times 0.1 + 1\times 0.6 + 2 \times 0.3 \\
			&= 1.2
		\end{align*}
	\end{exmp}
	\begin{exmp}
		Let $X$ be a discrete random variable such that 
		$$ P_X =
			\begin{cases}
				0 & n \in \N \setminus \{0\} \\
				\frac{k}{n^2 + n} & n \in \N
			\end{cases}
		$$
		Then:
		\begin{align*}
			k= \frac{1}{\sum_{n=1}^{\infty} \frac{1}{n^2 + n}} &\implies \sum_{n=1}^N \left[\frac{1}{n} -\frac{1}{n+1} \right] = 1 - \frac12 + \frac12 - \frac13 + \dots - \frac{1}{N+1} \\
			&\implies \sum_t |t| P_X(t) = \sum_{n=1}^\infty \frac{1}{n+1} \text{ diverges}
 		\end{align*}
 		$$ \therefore \E(X) \text{ is not defined. }$$
	\end{exmp}
	\begin{prop} \label{prop:3.2}
		Let $X$ be a discrete random variable, and $P_X$ its probability function. Let $g: \R \to \R$ be a function, and $Y= g \circ X = g(X)$. Then $Y$ is a discrete random variable as well.
		\begin{figure}[h]
			\begin{tikzpicture}
				\draw (0,0) ellipse (1cm and 2cm) 
					node[above=1cm, left=1cm]{\Large$\Omega$}
				;
				\draw
				 	(2, -2) -- (3, 2) node[below=3.2cm]{\Large$\R$}
				;
				\draw
					(4, -2) -- (5, 2) node[below=3.2cm]{\Large$\R$}
				;
				\draw 
					[->,  >=stealth, auto ] (0,0) to [out=20, in = 160] (2.5, 0) node[above=5mm, left=8mm]{$X$} ;
				\draw 
					[->,  >=stealth, auto ] (2.5,0) to [out=20, in = 160] (4.5, 0) node[above=5mm, left=8mm]{$g$} ;
			\end{tikzpicture}
		\end{figure}
	\end{prop}
	\begin{cor}
		$$ \E(Y) = \E \big( g(X) \big) = \sum_t g(t) P_X (t)$$
	\end{cor}
	\begin{exmp}
		Consider the following probability table:
		\begin{table}[h]
			\begin{tabular}{c|c|c|c}
				t & 0        & 1 & 2        \\ \hline
				$P_X(t)$ & $\frac12$ & $\frac13$ & $\frac23$
			\end{tabular}
		\end{table}
	\end{exmp}
	Set $g(X) = X^2 = Y$, so the probability table becomes:
	\begin{table}[h]
		\begin{tabular}{c|c|c}
			s & 0        & 4        \\ \hline
			$P_X(s)$ & $\frac12$ & $\frac12$
		\end{tabular}
	\end{table}
	By proposition (\ref{prop:3.2}) we have that
	\begin{align*}
		\E(Y) &= \E(X^2) \\
		&= \sum_t t^2 P_X (t) \\
		&= (-2)^2 + 0^2 \cdot \frac12 + (2)^2 \cdot \frac13 \\
		&= 4 \left( \frac16 + \frac13 \right) \\
		&= 2
	\end{align*}
	
	Recall that if $X$ is a discrete random variable, then $X$ is uniquely defined by its probability function:
	$$ \E \big( f(t_0) \big) = \sum_{t_0} f(t_0) P_X (t_0)$$
	
	We now present some properties of the expected value:
	\begin{enumerate}[$\quad\quad$(1)]
		\item If $f: \R \to \R$ is a function such that $f(t_0) = k \quad \forall t_0 \in \R$ ($k$ is a constant), then$$ \big( f(X) \big) = \E(k) = k$$
		\item Let $f, g$ be 2 functions. Then
		$$ \E \big( f(X) +g(X) \big) = \E \big( f(X) \big) + \E \big( g(X)\big)$$
		\item For $\alpha \in \R$,
		$$ \E \big( a f(X) \big) = \alpha \E\big( f(x)\big)$$
	\end{enumerate}
	\begin{proof}
		\emph{Left as an exercise to the reader.}
		5
	\end{proof}
\vspace{2cm}
	\begin{defn}
		Let $X$ be a random variable, and let $\mu_X = \E (X)$. The \textbf{variance} of $X$ is defined as
		$$ \V = \E \left[(X - \mu_X)^2\right]$$
	\end{defn}
\pagebreak
	\begin{exmp}
		Consider the following probability table:
		\begin{table}[h]
			\begin{tabular}{c|c|c|c}
				t & 0        & 1 & 2        \\ \hline
				$P_X(t)$ & $0.1$ & $0.6$ & $0.3$
			\end{tabular}
		\end{table}
	
		Here, $\mu_X = (0)(0.1) + (1)(0.6)+(2)(0.3) = 1.2$, and so:
		\begin{align*}
			\V(X) &= \E \left[(X - \mu_X)^2\right] \\
			&= \E \left[ (X -1.2)^2 \right] \\
			&= \sum_{t_0} (t_0 - 1.2)^2 P_X(t_0) \\
			&= (1.2)^2 (0.1) + (0.2)^2 (0.6) + (0.8)^2 (0.3) \\
			&= 0.36
		\end{align*}
	\end{exmp}
	\begin{prop}
		$$ \V(X) = \E(X^2) - \big( \E(X) \big)^2$$
	\end{prop}
	\begin{proof}
		\begin{align*}
			\V(X) &= \E \big( (X-\mu_X)^2 \big) \\
			&= \E \left[ X^2 - 2 \mu_X X + \mu_X^2 \right] \\
			&= \E (X^2) - 2 \mu_X \E(X) + \E(\mu_X^2) \\
			&= \E(X^2) - 2 \mu_X^2 + \mu_X^2 \\
			&= \E (X^2) - \big( \E(X) \big)^2
		\end{align*}
	\end{proof}
	\begin{exmp}
		\begin{align*}
			\E(X^2) &= (0)^2 (0.1) + (1)^2 (0.6)  + (2)^2 (0.3) \\
			&= (0.6) + (1.2) \\
			&= 1.8\\
			\therefore \V(X) &= 1.8 - (1.2)^2 \\
			&= 0.36
		\end{align*}
	\end{exmp}
\pagebreak
	\begin{exe}
		\textbf{Discrete Uniform Distribution} \\
		Let $k$ be a fixed positive number. Let $t_1, t_2, \dots, t_k \in \R$ be a uniform discrete distribution, with support $\{X_1, X_2,  \dots, X_k\}$, where $X_i$ is a random variable with a probability function:
		$$ P_{X_i} (t_0) = 
			\begin{cases}
			\frac{1}{k} & t_0 \in \{t_1, t_2, \dots, t_k \} \\
			0 & t_0 \notin \{t_1, t_2, \dots, t_k \}
			\end{cases}
		$$
		Prove that 
		\begin{enumerate}[$\quad\quad$(1)]
			\item$ \E(X) = \bar{t_0} = \frac{1}{k} \sum_{i=1}^{k}({t_0})_i$ 
			\item $\V(X) = \frac{1}{k} \sum_{i=1}^k \big( (t_0)_i - \bar{t_0} \big)^2$
		\end{enumerate}
	\end{exe}
	Here are some properties for the variance $\V(X)$:
	\begin{enumerate}[$\quad\quad$(1)]
		\item $\V (X + \beta) = \V (X)$ 
		\item $\V (\alpha X)  = \alpha^2 \V(x)$
	\end{enumerate}
\subsection{Types of Discrete Random Variables}
	\subsubsection{Bernoulli Random Variables}
	We consider an experiment with two outcomes, such as a coin toss with may not be fair. Let $\Omega = \{H,T\}$  such that $$P(H) = p \in (0,1)$$ without loss of generality. Then, define a discrete random variable $X$ such that $X(H) = 1$ and $X(T) = 0$. Then, the probability of $X$ is given by the following probability table:
	\begin{table}[h]
		\begin{tabular}{c|c|c}
			$X$ & 1      & 0     \\ \hline
			$P_X(t)$ & $p$ & $1-p$
		\end{tabular}
	\end{table}

	Thus, $X$ has a \textbf{Bernoulli Distribution}, with parameter $p$ such that
	$$ X \sim \text{Ber}(p)$$ 
	\begin{exe}
		Show that if $X \sim \text{Ber}(p)$, then $Y = 1- X \sim \text{Ber}(1-p)$.
	\end{exe}
	\begin{exe}
		Consider a random variable $X$ with a probability function with $a,b \in \R$ fixed such that the following probability table holds:
		\begin{table}[h]
			\begin{tabular}{c|c|c}
				$X^*$ & a     & b     \\ \hline
				$P_X(X^*)$ & $p$ & $1-p$
			\end{tabular}
		\end{table}
	
		Show that $Y = \frac{X-b}{a-b} \sim \text{Ber}(p)$ where $X = (a-b) Y + b$ and $Y \sim Ber(p)$.
	\end{exe}
\pagebreak
	\begin{prop}
		Let a discrete random variable be $X \sim \text{Ber}(p)$, then:
		\begin{enumerate}[$\quad\quad$(1)]
			\item $\E(X) = p$ 
			\item $\V(X) = p(1-p)$
		\end{enumerate}
	\end{prop}
	\begin{proof}
		\begin{enumerate}[$\quad\quad$(1)]
			\item $\E(X) = 1(p) + 0 (1-p) = p$ 
			\item $\E(X^2) = 1(p)+0(1-p) = p  \implies \V (X) = p - p^2 = p(1-p)$
		\end{enumerate}
	\end{proof}
	\begin{rem}
		\begin{align*}
			\E(X) &= \E \big( (a-b Y + b) \big) \\
			&= (a-b) \E(Y) +b \\
			&= (a-b) p +b \\
			\V(X) &= \V\big( (a-b) Y +b \big) \\
			&= V \big( (a-b) Y \big) \\
			&= (a-b)^2 \V(Y) \\
			&= (a-b)^2 p (1-p)
 		\end{align*}
	\end{rem}
	\subsubsection{Binomial Distribution}
		Assume that we have a Bernoulli trial experiment with two possible events $A$ and $B$. Here:
		\begin{align*}
			P(A) &= p \in (0,1) \\
			P(B) &= 1-p
		\end{align*}
		We repeat the trial experiment $k\in \N$ times, and let $X = \{1, 2, \dots, k\}$ be the number of events $A$ observed. The random variable $X$ is then said to have a \textbf{Binomial distribution} with parameters $k, p$, denoted:
		$$ X \sim \text{Bin}(k, p)$$
		\begin{prop}
			For a Bernoulli trial with $k\in \N$ repetitions such that $n \in \N \setminus (k, \infty)$ and \\$P(A) = p \in (0,1)$:
			$$ P_X(X=n) = C_n^k p^n (1-p)^{k-n}$$
		\end{prop}
		\begin{proof}
			We start by assuming that the trials are independent from each other. Then,
			\begin{align*}
				P_X(0) &= P(X=0)  \\
				&= P(\underbrace{B B B \dots B}_{k \text{ times}})  \\
				&= (1-p)^k \\
				P_X(1) &= P(X=1) \\
				&= P\big( (A\underbrace{BB\dots B}_{k-1 \text{ times}})  \cap (BA\underbrace{B\dots B}_{k-2 \text{ times}}) \cap \dots \cap (\underbrace{BB\dots B}_{k-1 \text{ times}}A)\big) \\
				&= k p (1-p)^{k-1} \\
				P_X(2) &= P(X=2) \\
				&= P\big( (AA\underbrace{B\dots B}_{k-2 \text{ times}})  \cap (BAA\underbrace{B\dots B}_{k-3 \text{ times}}) \cap \dots \cap (\underbrace{B\dots B}_{k-2 \text{ times}}AA)\big) \\
				&= C_2^k p^2 (1-p)^{k-2}\\
				&\,\, \, \vdots\\
				P_X(n) &= P(X=n)  \\
				&= C_n^k p^n (1-p)^{k-n}
 			\end{align*}
		\end{proof}
		\begin{prop}
			If $X \sim \text{Bin}(k, p)$ then
			\begin{enumerate}[$\quad \quad$ (1)]
				\item $\E (X) = kp  $
				\item $\V(X) = kp (1-p)$
			\end{enumerate}
			\begin{proof}
				\begin{enumerate}[$\quad\quad$(1)] 
					\item 
						\begin{align*}
							\E(X) &= \sum_{m=1}^k  m P_X (m) \\
							&= \sum_{m=1}^k m C_m^k p^m (1-p)^{k-m} \\
						\intertext{Recall that $x C_x^{y} =x \frac{y!}{x!(y-k)!} = y C_{x-1}^{y-1}$, so }
							E(X) &= \sum_{m=1}^k C_{m-1}^{k-1} p^m (1-p)^{k-m} \\
							&= k \sum_{\ell = 0}^k-1 C_\ell^{k-1} p^{\ell + 1} (1-p)^{k-1-\ell} \\
							&= k p \sum_{\ell = 0}^{k-1} C_\ell^{k-1} p^\ell (1-p)^{(k-1)-\ell} \\
							&= kp \big( p + (1-p) \big)^{k-1} \\
							&= kp
						\end{align*}
					\item Remark that $X^2 = X(X-1)+X$. Thus, 
					\begin{align*}
						\V(X) &= \E(X^2) - (kp)^2 \\
						&= E \big( X(X-1) \big) + kp - k^2 p^2 \\
						\therefore E \big( X (X-1)\big) &= \sum_{m=0}^k m(m-1) C_m^k p^m (1-p)^{k-m} \\
						&= \sum_{m=2} m (m-1) C_m^k p ^m (1-p)^{k-m} \\
						\therefore m (m-1) C_m^k&= m (m-1) \frac{k!}{m! (k-m)!}   \\
						&= k(k-1) C_{m-1}^{k-2} \\
						\therefore \E \big( X(X-1) \big) &= k(k-1) \sum_{m=2}^k C_{m-2}^{k-2} p^m (1-p)^{k-m} 
						\intertext{Let $b = m-2$, then:}
						 \E \big( X(X-1) \big)  &= k (k-1) \sum_{b=0}^{k-2} C_b^{k-2} p^{b+2} (1-p)^{k-2-b} \\
						 &= k (k-1) p^2 \sum_{b=0}^{k-2} C_{b}^{k-2} p^{b} (1-p)^{k-2} \\
						 &= k (k-1) p^2 \\
						 \therefore \V(X) &= k(k-1)p^2 + kp - (kp)^2\\
						 &= -kp^2 + kp\\
						 &= kp (1-p)
					\end{align*}
				\end{enumerate}
			\end{proof}
		\end{prop}
		\begin{exe}
			Let $X \sim \text{Bin}(k, p)$. Find $\E(X^3)$. \\
			({\footnotesize \emph{Hint: $X^3 = \frac{X(X-1)(X-2)}{3X^2 - 2X}$}})
		\end{exe}
		\begin{rem}
			$X \sim \text{Bin}(k, p) \iff Y = k - X \sim \text{Bin} (k, 1-p) $
		\end{rem}
		\begin{exmp}
			An oil firm undergoes 10 explorations. The probability of a succesful exploration is 0.1. 
			$$ \text{cost} = 
			\begin{cases}
				\$20,000 &\text{Equipments} \\
				\$ 30,000 &\text{for each succesful exploration}\\
				\$ 15,000 &\text{for each failed exploration}
			\end{cases}
			$$
			Find the mean and the  variance of the (total) cost for the firm.
			
			\begin{sol}
				Set a random variable $X=$ ``number of succesful explorations" such that $X \sim $Bin$(10,0.1)$. Let $Y=$cost$_{\text{Tot}}$. Then:
				\begin{align*}
					Y &= 2(10^4) + 3(10^4) + 15(10^3) (10-X) \\
					&= 15,000X + 170,000 \\
					\therefore \E(Y) &= 15,000 \E(X) + 170,000 \\
					&= 15,000 kp + 170,000 \\
					&= 185,000 \quad (kp=1)\\
					\therefore \V(Y) &= \big(15(10^3)\big)^2 \V (X)\\
					&= 225(10^6) kp (1-p) \\
					&= 2025 (10^5) \\
				\end{align*}
				\begin{note}Variance refers to the \emph{spread} of the data given.
				\end{note}
			\end{sol}
		\end{exmp}
	\pagebreak
	\subsubsection{Geometric Distribution}
	Recall the \emph{Geometric series:} 
	$$ \sum_{k=0}^\infty x^k = \frac{1}{1-x} \quad \forall |x| <1$$
	Consider a Bernoulli trial. We have two events, $A$ with probability $P(A)=p$, and $B$ with probability $P(B)= 1-p$. We repeat the trial until the \emph{first} instance of $A$ occurs.
	
	\begin{exmp}
		Roll a die unil 2 appears. Let $X =$ `` Number of trials until success (assuming that trials are independent).
		$$ X = \{ 1,2,3, \dots\} = \N^* = \N\setminus \{0\} $$
		The probability function of $X$ is 
		\begin{align*}
			p(k) &= P(X=k) \\
			&= P\big( (\underbrace{BBB\dots B}_{k-1}A)\big) \\
			&= (1-p)^{k-1} p \quad\quad  k \in \N^*
		\end{align*}
		$X$ is said to have a geometric distribution with parameter $p$, denoted
		$$ X \sim \text{Geo}(p)$$
	\end{exmp}
	\begin{exe}
		\textbf{Memoryless Property} \\
		Given $k, \ell \in \N^*$ with $X \sim \text{Geo (p)}$ prove that
		$ P (t > k + \ell : \quad > \ell) = P(X>k)$
	\end{exe}
	\begin{exmp}
		Consider
		\begin{align*}
			P(X > n_0) &= P(X=n_0+1) +P(X=n_0 + 2)+ \dots\\ 
			&= \sum_{k=n_0+1}^\infty P(X=k) \\
			&= \sum_{k=n_0 + 1}^\infty (1-p)^{k-1}p \\
			&= p \sum_{k=n_0 +1}^\infty (1-p)^{k-1}\\
			&= p \sum_{\ell =n_0}^\infty (1-p)^\ell
		\intertext{Let $\ell = k-1$,}
		P(X > n_0) &= p(1-p)^{n_0} \sum_{\ell = n_0}^\infty (1-p)^{\ell - n_0} 
		\intertext{Let $m = \ell - n_0$,}
		&= p(1-p)^{n_0} \sum_{m=0}^\infty (1-p)^m\\
		&= (1-p)^{n_0}
		\end{align*}
		The cumulative distribution function of $X$ is 
		\begin{align*}
			F_X (n_0) &= P(X \leq n_0) \\
			&= 1- P(X > n_0) \\
			&= 1- (1-p)^{n_0} 
		\end{align*}
	\end{exmp}
	\begin{prop}
		Assume that we have $X \sim $Geo$(p)$, then:
		\begin{enumerate}[$\quad\quad$(1)]
			\item $\E(X) = \frac{1}{p} $
			\item $\V (X) = \frac{1}{p}\left( \frac{1}{p} - 1 \right)$
		\end{enumerate}
	\end{prop}
	\begin{proof}
		Recall that 
		$$ \forall |x| < 1 : \quad \frac{1}{1-x}= \sum_{k=0}^\infty x^k \quad \implies $$
		Thus, 
		\begin{align*}
			\E(X) &= \sum_{k=1}^\infty k P (X= k) \\
			&= \sum_{k=1}^\infty k (1-p)^{k-1}p \\
			&= p \frac{1}{\big(1- (1-p) \big)^2} \\
			&= \frac{p}{p^2} \\
			&= \frac1p
		\end{align*}
		Now 
		\begin{align*}
			\V(X) &= \E(X^2) - \frac{1}{p^2}  			
			\intertext{Recall that $\E(X^2) = \sum_{k=1}^\infty k^2 (1-p)^{k-1}p $ and $X^2 = X(X-1)+X$, so } 
			\V(X) &= E\big( X(X-1) \big) + \frac1p - \frac{1}{p^2} 
			\intertext{Remark that}
			\E\big(X(X-1) \big) &= \sum_{k=2}^\infty k (k-1) (1-p)^{k-1} p\\
			&= \frac{2(1-p)}{p^2} \\
			\therefore \V(X) &= \frac{2}{p^2} - \frac{2}{p} + \frac{1}{p} - \frac{1}{p^2} \\
			&= \frac{1}{p^2} - \frac{1}{p} \\
			&= \frac{1}{p} \left( \frac{1}{p} -1\right)
		\end{align*}
	\end{proof}
	
	\subsubsection{Hypergeometric Distribution}
	Consider a sample space $\Omega$ of population $\omega$ with a subset $S \subset \Omega$ of size $\sigma$. We take a random sample of size $m$ and let $X$ be the number of subjects from the subset $S$ included in the random sample.
	
	\begin{exmp}
		An urn contains 6 red balls, and 4 blue balls. We select a random sample of 7 balls. Let $X = $``number of blue balls in the sample" such that $\omega=10$, $\sigma = 4$, and $m = 7$. Thus,  $\omega - \sigma = 6$.  
		\begin{note}
			$X \leq \sigma$ and $X \leq m$, meaning that $	X \leq \min(m, \sigma)$. Also, 
			$$ (m - X \leq m) \bigcap (m-X \leq \omega - \sigma ) \implies m \geq \max(0, m+\sigma - \omega)$$
		\end{note}
		Thus, $X$ is a random integer such that 
		$$ \max(0, m + \sigma - \omega) \leq X\leq \min (m, \sigma)$$
		The probability function of $X$ is found by letting $k \in \Z$ such that
		$$ \max(0, m + \sigma - \omega) \leq k\leq \min (m, \sigma)$$
		Thus:
		\begin{align*}
			P_X (k) &= P(X = k) \\
			&= \frac{C_k^\sigma C_{m-k}^{\omega - \sigma}}{C_m^\omega}
		\end{align*}
		$X$ is said to have a \textbf{hypergeometric distribution} with parameters $\omega, m, \sigma$ and denoted:
		$$ X \sim \text{HyGeo}(\omega, m, \sigma)$$
	\end{exmp}
	\begin{prop}
		If $X \sim $HyGeo$(\omega, m, \sigma)$, then
		\begin{enumerate}[$\quad\quad$(1)]
			\item $\E(X) = m \frac{\sigma}{\omega}$ 
			\item $\V(X) = m \left( \frac{\sigma}{\omega}\right) \left(1- \frac{\sigma}{\omega}\right)\big( \frac{\omega-m}{\omega-1} \big) $
		\end{enumerate}
	\end{prop}
	\subsubsection{Poisson Distribution}
		Recall that 
		$$ e^x = \sum_{n=0}^\infty \frac{x^n}{n!} \quad\quad \forall x \in \R$$
		We will come up with a probability function $P(X)$ such that 
		\begin{enumerate}[$\quad\quad$(1)]
			\item $0 \leq P(X) \leq 1$ $\quad \forall X$
			\item $\sum_{\omega \in \Omega} P(\omega) \in 1$
		\end{enumerate}
		Thus, we define:
		$$ P(\omega) = 
			\begin{cases}
				0	&\quad	\omega \notin \N \\
				e^{-t}\left( \frac{t^\omega}{\omega!} \right)&\quad \omega \in \N
			\end{cases}
		$$
		Note that $0\leq P(X) \leq 1 \quad \forall  X$ , the set $\{ t \in \R: P(t) \neq 0\}$ is discrete, and $$\sum_t P(t) = 1$$
		Therefore, $P$ is the probability function of some discrete random variable $X$ which is said to have a \textbf{Poisson distribution} with a parameter $t$, denoted
		$$ X \sim \text{Poisson(t)}$$
		For the next two examples, assume a discrete random variable $X\sim$Poisson$(t=3.2)$, where $X$ represents the number of calls required at a call center during a given day. 
	\pagebreak
		\begin{exmp}
			What is the probability that the call center receives more than 2 calls?
			\begin{sol}
				\begin{align*}
					P(X > 2) &= 1 - P(X \leq 2) \\
					&= 1 - \big( P(0) + P(1) + P(2) \big) \\
					&= 1 - \left( e^{-t} \left( \frac{t^0}{0!} + \frac{t^1}{1!} + \frac{t^2}{2!} \right) \right)
				\end{align*}
			\end{sol}
		\end{exmp}
		\begin{exmp}
			What is the most likely number of calls received?\\
			(i.e. for which $n$ in $P(n) = e^{-t} \frac{t^n}{n!}$ is maximal. We measure)
			$$ \frac{P(n+1)}{P(n)} = \frac{t}{n+1}$$
			Thus, 
			\begin{align*}
				\frac{t}{n+1} \geq 1 &\iff n+1 < t \\
				&\iff n < t-1 = 2.2 \\
				\frac{t}{n+1} < 1 &\iff n > 2.2 
			\end{align*}
			Thus, the maximal value is $n=3$ such that $P(3)=e^{-t} \frac{t^3}{3!}$
			\begin{figure}[h]
				\begin{tikzpicture}
					% Axes
					\draw [->, >=stealth] (0,0) to (0,4) ;
					\draw [->, >=stealth] (0,0) to (6,0) ;
					\draw (-0.1, 3) -- (0.1, 3) node[left=2mm]{1};
					\foreach \x in {0, ..., 5}
						\draw (\x, -0.2) node{\x};
					% Data
					\draw [ultra thick] (0,0) -- (0,1.3) node{$\bullet$};
					\draw [ultra thick] (1,0) -- (1,1.8) node{$\bullet$};
					\draw [ultra thick] (2,0) -- (2,2.6) node{$\bullet$};
					\draw [ultra thick] (3,0) -- (3,3) node{$\bullet$};
					\draw [ultra thick] (4,0) -- (4,2.7) node{$\bullet$};
					\draw [ultra thick] (5,0) -- (5,2.4) node{$\bullet$};
				\end{tikzpicture}
			\end{figure}
		\end{exmp}
		\begin{exe}
			Let $X_n$  for $n=1,2$ be the number of cars going through the entrance $n$ in a parking lot on any given day. Assume that $X_1$ and $X_2$ are independent such that the events $\{X_1 = k\}$ and $\{X_2 = \ell\}$ are independent $\forall k, \ell$. Prove that
			$$ X_i \sim \text{Poisson}(t_i), \quad i =1,2 \implies X = X_1 + X_2$$
		\end{exe}
		\begin{sol}
			Let $X \in \{ 0, 1, 2,\dots\} = \N$ with
			\begin{align*}
				P(0) &= P\left( X_1 =0 \bigcap X_2 = 0 \right) \\
				&= P(X_1 = 0) \cdot P (X_2 =0) \\
				&= e^{-t_1} \cdot e^{-t_2} \\
				&= e^{-(t_1 + t_2)} \\
				P(1) &= P \left[ \left( X_1 = 1 \bigcap X_2 = 0 \right) \bigcup \left( X_1 = 0 \bigcap X_2 = 1 \right) \right] \\
				&= P\left( X_1 = 1 \cap X_2 = 0 \right) + P(X_1 = 0 \cap X_2 = 1) \\
				&= e^{-t_1} t_1 e^{-t_2} + e^{-t_1}e^{-t_2} t_2 \\
				&= (t_1 + t_2 )e^{- (t_1 + t_2)}
			\end{align*}
		\end{sol}
		\begin{prop}
			If $X \sim $Poisson$(t)$, then
			\begin{enumerate}[$\quad\quad$(1)]
				\item $\E(X) = t$ 
				\item $\V(X) = t$
			\end{enumerate}
		\end{prop}
		\begin{proof}
			\begin{enumerate}[$\quad \quad $(1)]
				\item \begin{align*}
					\E(X) &= \sum_{n=0}^\infty n e^{-t} \frac{t^n}{n!} \\
					&= \sum_{n=1}^\infty e^{-t} \frac{t^n}{(n-1)!} \\
					&= t e^{-t} \sum_{n=1}^\infty \frac{t^{n-1}}{(n-1)!} 
					\intertext{Let $(k= n-1)$ such that:}
					\E(X) &= t e^{-t} \sum_{k=0}^\infty \frac{t^k}{k!} \\
					&= t
				\end{align*}
				\item 
				\begin{align*}
					\V(X) &= \E\big(X (X-1)\big) + \E (X) - \big(\E(X)\big)^2 \\
					&= \E \big( X (X-1) \big) + t - t^2 \\
					&= t
				\end{align*}
			\end{enumerate}
		\end{proof}
		\begin{exe}
			Find $\E(X^3)$
		\end{exe}
\subsection{Moment Generating Function}
	\begin{defn}
		Given a random variable $X$, a \textbf{moment generating function} of $X$ is the function defined as
		$$ m_X (t) = \E (e^{tX})$$
		More precisely, for  discrete random variable we have
		$$ m_X (t) = \sum_{X} e^{tX} P_X (k)$$
		Here are some of its properties:
		\begin{enumerate}[$\quad \quad$(1)]
			\item $m_X$ is always defined for $t=0$:
			\begin{align*}
					m_X(0) &= \E \left( e^{0X} \right) \\
					&= \E (1) \\
					&= 1
			\end{align*}
			\item The domain of $m_X$ is always an interval which contains zero.
			\item $m_X$ is a unique identifier of a distribution:
			$$ m_X \equiv m_Y$$
		\end{enumerate}
	\end{defn}
	\begin{exmp}
		Consider the following probability table:
		\begin{table}[h]
			\begin{tabular}{c|c|c|c}
				$X$ & 0     & 1    & 2 \\ \hline
				$P_X(X^*)$ & $0.1$ & $0.6$ &$0.3$
			\end{tabular}
		\end{table}
	
		The moment generating function $mgf$ of $X$ is
		\begin{align*}
			m_X (t) &= \E (e^{tX}) \\
			&= \sum_X e^{tX} p_X (X) \\
			&= 0.1(e^{t0}) + 0.6(e^{t1}) + 0.3(e^{t2})  \quad \forall t \in \R\\
 		\end{align*}
 	\end{exmp}
 	\begin{exmp}
 		Consider the following probability table:
 		\begin{table}[h]
 			\begin{tabular}{c|c|c|c|c}
 				$X$ & -5    & -2    & 0 & 4 \\ \hline
 				$P_X(X^*)$ & $\frac17$ & $\frac47$ &$\frac27$ & $\frac17$
 			\end{tabular}
 		\end{table}
		\begin{align*}
			m_X (t) &= \E (e^{tX}) \\
			&= \E \left[ \sum_{n=0}^\infty \frac{(tX)^n}{n!} \right]\\
			&= \E \left[ \sum_{n=0}^{\infty} X^n \frac{t^n}{n!}\right] 
		\intertext{Treading very carefully, we have:}
			m_X (t) &= \sum_{n=0}^\infty \frac{\E (X^n)}{n!} t^n  \quad \forall t \in \R \\
		\end{align*}
		Here, $m_X(t) $ is the \emph{Maclaurin Series} of $mgf$. It follows that:
		$$ 
			\underbrace{\E (X^n)}_{n^{th} \text{ moment  of } X} = \frac{d^n}{dt^n} (m_X(t)) \bigg|_{t=0}
		$$
	\end{exmp}
	\begin{prop}
		Let $X$ be a random variable such that the domain $D$ of the $mgf$ contains an open interval centered at 0, then
		$$ \E (X^n) = \frac{d^n}{dt^n} \left( m_x (t) \right) \bigg|_{t=0}$$
	\end{prop}
	\begin{exmp}
		Consider the following probability table:
		\vspace{1cm}
		\begin{table}[h]
			\begin{tabular}{c|c|c|c}
				$X$ & 0    & 1    & 2 \\ \hline
				$P_X(X^*)$ & $\frac{1}{10}$ & $\frac{6}{10}$ &$\frac{3}{10}$ 
			\end{tabular}
		\end{table}
	
	Thus:
	\begin{align*}
		m_X (t) &= \frac{1}{10} e^{t0} + \frac{6}{10} e^{t1} + \frac{3}{10} e^{t2} \\
		\therefore D &= \R = (-\infty, +\infty)
		\intertext{Hence, 0 is an interior point. It follows that:}
		\E (X^n) &= \frac{d^n}{dt^n} m_X (t) \bigg|_{t=0} \\
		\therefore \frac{d}{dt} m_X (t) &= \frac{6}{10} e^t + \frac{6}{10} e^{2t} \\
		\frac{d}{dt} m_X (0) &= \frac{6}{10}  + \frac{6}{10} = \frac{6}{5} = \E (X)\\
		\frac{d^2}{dt^2} m_X (0) &= \frac{6}{10}  + 2\cdot\frac{6}{10} = \frac{9}{5} = \E (X^2) 
	\end{align*}
	Therefore:
	$$ \V(X) = \E \big( X (X-1) \big) + \frac{6}{5} - \frac{9}{5}$$
	\end{exmp}
	We can now analytically redefine the \emph{Moment Generating Function} $mgf$ such that:
	$ m_X (t) = \E (e^{tX})$ is defined from $(- \varepsilon, \varepsilon)$ for $\varepsilon >0$. Then, 
	$$ \E(X^n) = \frac{d^n}{dt^n} \big( m_X(t) \big) \bigg|_{t=0}$$
	
	\begin{exmp}
		\textbf{\emph{mgf} of a Binomial}\\
		Let $X \sim$Bin$(n, p )$ such that
		\begin{align*}
			P(X=k) &= C_k^n p^k (1-p)^k \quad \quad \quad k = 0,1,2,\dots, n \\
		\intertext{Thus:} 
			m_X(t) &= \E (e^{tX}) \\
			&= \sum_{k=0}^{n} e^{tk} C_k^n p^k (1-p)^{n-k} \\
			&= \sum_{k=0}^{n} C_k^n (pe^t)^k (1-p)^{n-k} \\
			&= \big( pe^t + (1-p) \big)^n \quad \forall t \in \R \\
		\therefore 	\frac{d}{dt} m_X (t)&= npe^t(pe^t + (1-p))^{n-1}
		\end{align*}
		\begin{align*}
		\intertext{Let $t=0$, so}
			\frac{d}{dt}m_X (0) &= np = \E (X)
		\intertext{Now,}
			\frac{d^2}{dt^2} m_X (t) &= npe^t (pe^t + (1-p))^{n-1} + n(n-1)p^2 e^{2t} (pe^t + (1-p))^{n-2} \\
			\therefore 	\frac{d^2}{dt^2} m_X (0) &= np + (n)(n-1) p^2 = \E(X^2)
		\intertext{We may now find the variance:}
			\V(X) &= \E(X^2) - \big(\E(X))\big)^2 \\
			&= np + (n)(n-1) p^2 - (np)^2 \\
			&= np + (n^2  - n)p^2 - (np)^2 \\
			&= np - \cancel{(np)}^2 - np^2 - \cancel{(np)}^2 \\
			&= np (1-p) \\
		\end{align*}
	\end{exmp}
	\begin{exe}
		Find $\E(X^3)$ and compare it with your previous answer. 
	\end{exe}
	\begin{exmp}
		Start with a fortune $m=100$. Toss a coin such that $P(H) = p \in (0,1)$. If the result is $H$, the fortune doubles. If the result is $T$, the fortune is halved. What will be the expected fortune after $n$ tosses? Let us denote the random variables $Y$ as the fortune, and $X$ the number of $H$ after $n$ tosses. Then, 
		\begin{align*}
			X &\sim \text{Bin}(n, p) \\
			Y &= m2^{X} \left( \frac{1}{2}\right)^{n-X}
		\end{align*}
		Hence, 
		\vspace{1.5cm}
		\begin{align*}
			Y &= \frac{100}{2^n} 2^{2X} \\
			&= \frac{100}{2^n} 4^X \\
		\end{align*}
		\begin{align*}
			\therefore \E(Y) &= \frac{100}{2^n} \E(4^X) \\
			&= \frac{100}{2^n}  \E(e^{\ln(4)X}) \\
			&= \frac{100}{2^n} m_X (t = \ln (4)) \\
			&= \frac{100}{2^n} (4p + 1 - p )^n  \\
			&= \frac{100}{2^n} (3p+1)^n
		\end{align*}
	\end{exmp}
	\begin{exmp}
		\textbf{\emph{mgf} of a Geometric random variable}\\
		Let $X \sim \text{Geometric} (p)$, and recall that $X \in \{ 1, 2, \dots\} = \N \setminus \{0\}$ . Thus,
		\begin{align*}
			P(X=k) &= (1-p)^{k-1} (p) \\
			\therefore m_X (t) &= \E (e^{tX}) \\
			&= \sum_{k=1}^\infty e^{tk} P(X=k) \\
			&= \sum_{k=1}^{\infty} e^{tk} p(1-p)^{k-1} 
		\intertext{Let $k=n+1$, then }
			m_X (t)&= \sum_{n=0}^\infty e^{t (n+1)} (1-p)^n p \\
			&= pe^t \sum_{n=0}^{+\infty} \left[ (1-p) e^t\right]^n \\
			&= \frac{pe^t}{1-(1-p)e^t} \quad\quad (\text{if } (1-p)e^t < 1) \\
		\end{align*}
		Therefore, 
		\vspace{1.5cm}
		$$ (1-p)e^t < 1 \iff t < -\ln (1-p)$$
		$$ \therefore t < -\ln(1-p) \iff m_X (t) = \frac{pe^t}{1-(1-p)e^t}$$
	\end{exmp}
\pagebreak
	\begin{exmp}
		\textbf{\emph{mgf} of a Poisson random variable}\\
		Let $X \sim $ Poisson$(\lambda)$. Thus, we have that 
		$$ P(X=k) = e^{-\lambda} \frac{\lambda^k}{k!}, \quad \quad k \in \N$$
		\begin{align*}
			\therefore m_X (t) &= \E \left( e^{tX} \right) \\
			&= \sum_{k=0}^{+\infty} e^{tk} e^{-\lambda} \frac{\lambda^k}{k!}  \\
			&= e^{-\lambda} \sum_{k=0}^{\infty} \frac{(\lambda e^t)^k}{k!} 
		\intertext{This series converges $\forall t$, hence:}
			m_X (t) &= e^{-\lambda} e^{\lambda e^t} \\
			&= e^{\lambda (e^t - 1)} 
		\end{align*}
		So,
		$$ m_X (t) = e^{\lambda(e^t - 1)} \quad \quad \forall t \in \R$$
		\begin{align*}
			\therefore \dot{m}_X(t) &= \lambda e^t e^{\lambda (e^t -1)} \\
			\dot{m}_X (0) &= \lambda = \E (X) \\
			\therefore \ddot{m}_X (t) &= \lambda e^t e^{\lambda (e^t - 1)} + \lambda^2 e^2t e^{\lambda (e^t -1)}\\
			\ddot{m_X(0)} &= \lambda + \lambda^2 \\
			&= \E(X^2)
		\end{align*}
		$$ \therefore \V (X) = \lambda + \lambda^2 - (\lambda)^2 = \lambda $$
	\end{exmp}
	\begin{exe}
		Find $\E (X^3)$
	\end{exe}
	\begin{exmp}
		The number of customers making a certain payment in a store is represented by the random variable $X$. Let $X \sim$ Poisson$(\lambda=2)$. We have the following distribution of payments per customer:
		\begin{table}[h]
			\begin{tabular}{|c|c|}
				\hline
				Customer & Payment\\ 
				\hline 
				$1^{st}$ & \$100.00 \\
				$2^{nd}$ & \$50.00 \\
				$3^{th}$ & \$25.00 \\
				$\vdots$ & $\vdots$ \\
				$X^{th}$ & \$100.00$\left( \frac{1}{2}\right)^{X-1}$ \\
				\hline
			\end{tabular}
		\end{table}
	
	Therefore, 
	\begin{align*}
		\E (\text{Cost}) &= \E \left( 100 \left( \frac{1}{2} \right)^{X-1} \right) \\
		&= \E \left( 50 (2 )^{-X} \right) \\
		&= 50 \cdot \E \left( 2^{-X} \right) \\
		&= 50 \cdot \E \left(  \left(\frac{1}{2} \right)^{X} \right) \\
		&= 50 \cdot \E \left(  e^{-X\ln(2)}\right)
	\end{align*}
	Thus, the \emph{mgf} of $X$ is
	\begin{align*}
		m_X (t) &= e^{\lambda (e^t - 1)} \quad \forall t \in \R \\
		&= \E (e^{tX}) \\
		&= 50 m_X (t = -\ln(2) ) \\
		&= 50 e^{\lambda (\frac{1}{2} - 1)} \\
		&= 50 e^{- \frac{\lambda}{2}}  
	\intertext{We know that $\lambda$ = 2, so}
		&= \frac{50}{e}
	\end{align*}
	
	\end{exmp}
\pagebreak
\section{Continuous Random Variables}
\subsection{Chebyshev Inequality}
	\begin{thm}
		Let $X$ be a random variable such that $\E(X) = \mu$ and $\V(X) = \sigma^2$. Then,
		$$ P\big( |X-\mu| \geq k \sigma \big) \leq \frac{1}{k^2} $$
		\vspace{-0.5cm}
		\begin{figure}[h]
			\begin{tikzpicture}
				\draw (-4, 0) -- (4, 0);
				\draw (-2.5, -0.1) -- (-2.5, 0.1) node[below=2mm]{$\mu - k \sigma$} ;
				\draw (0, -0.1) -- (0, 0.1) node[below = 2mm]{$\mu$} ;
				\draw (2.5, -0.1) -- (2.5, 0.1) node[below = 2mm]{$\mu+ k \sigma$} ;
				\draw [line width = 5pt, color=red!50, opacity = 0.5] (-4, 0) -- (-2.5, 0);
				\draw [line width = 5pt, color=green!50, opacity = 0.5] (-2.5, 0) -- (2.5, 0);
				\draw [line width = 5pt, color=red!50, opacity = 0.5] (2.5, 0) -- (4, 0);
			\end{tikzpicture}
		\end{figure}
	\end{thm}
	For instance, let $k=2$ such that 
	$$ P\big( |X-\mu|  \geq 2 \sigma \big) \leq \frac{1}{4}$$
	Hence, more than 75\% of the value of $X$ are within $2\sigma$ from $\mu$.
	\begin{rem}
		Let $A, B, C$ be events such that $A \indp B$, $B \indp C$, and $A \indp C$. Then, 
		$$ P(A \cap B \cap C) = P(A) P(B)P(C)$$
	\end{rem}
	\begin{defn}
		Recall the cumulative distribution function (\emph{cdf}) of a random variable $X$ is 
		$$F_X (t) = P(X \leq t) $$
		For example:
		\begin{figure}[h]
			\begin{subfigure}{0.45\textwidth}
				\begin{center}
					\begin{tikzpicture}
						\draw [ultra thin, color=grey!30] (-3, -3) grid (3,3);
						\draw [->] (-3, -1) -- (3, -1) node[right]{$x$} ; 
						\draw [->] (0, -3) -- (0, 3) node[above]{$y$} ; 
						\draw [thick] plot [smooth, tension =0.7] coordinates {(-3, -3) (-1.5, -2) (0, 2) (3, 3)};
					\end{tikzpicture}
					
					\emph{A continuous function.}
				\end{center}
				
			\end{subfigure}
			\hfill
			\begin{subfigure}{0.45\textwidth}
				\begin{center}
					\begin{tikzpicture}
					\draw [ultra thin, color=grey!30] (-3, -3) grid (3,3);
					\draw [->] (-3, -1) -- (3, -1) node[right]{$x$} ; 
					\draw [->] (0, -3) -- (0, 3) node[above]{$y$} ; 
					\draw [ ultra thick] (-3, -2) -- (-2.06, -2);
					\draw (-3,-2) node[]{$\bullet$} (-2, -2) node[]{$\circ$};
					\draw [ ultra thick] (-2, -1) -- (-1.06, -1);
					\draw (-2,-1) node[]{$\bullet$} (-1, -1) node[]{$\circ$};
					\draw [ ultra thick] (-1, 0) -- (-0.06, 0);
					\draw (-1,0) node[]{$\bullet$} (0, 0) node[]{$\circ$};
					\draw [ ultra thick] (0, 1) -- (0.94, 1);
					\draw (0,1) node[]{$\bullet$} (1, 1) node[]{$\circ$};
					\draw [ ultra thick] (1, 2) -- (1.94, 2);
					\draw (1,2) node[]{$\bullet$} (2, 2) node[]{$\circ$};
					\draw [ ultra thick] (2, 3) -- (2.94, 3);
					\draw (2,3) node[]{$\bullet$} (3, 3) node[]{$\circ$};
					\end{tikzpicture}
					
					\emph{A discrete function.}
				\end{center}
			\end{subfigure}
		\end{figure}
	
		A \textbf{continuous random variable} $X$ is a random variable for which the $cdf$  $F_X$ is a continuous function, in addition to the fact that $F_X$ is \emph{non-decreasing}.
		
		\begin{figure}[h]
			\begin{subfigure}{0.45\textwidth}
				\begin{center}
					\begin{tikzpicture}
					\draw [ultra thin, color=grey!30] (-3, -3) grid (3,3);
					\draw [->] (-3, -1) -- (3, -1) node[right]{$t$} ; 
					\draw [->] (0, -3) -- (0, 3) node[above]{$F_X(t)$} ; 
					\draw [thick] plot [smooth, tension =0.75] coordinates {(-3,-1)  (-1, -0.5) (1, 1.5) (3, 2)};
					\draw [dashed] (-3, 2) -- (3, 2) ;
					\draw [thick](-0.1, 2) -- (0.1, 2) node[right]{1};
					\draw (-0.1, -1) -- (0.1, -1) node[right=2mm, below]{0};
					\draw ;
					\end{tikzpicture}
				\end{center}
				
			\end{subfigure}
			\hfill
			\begin{subfigure}{0.45\textwidth}
				$$ 
					\left.
					\begin{array}{c}
						\lim\limits_{t \to - \infty} F_X (t) = 0 \\
						\\
						\lim\limits_{t \to + \infty} F_X (t) = 1 
					\end{array}
					\right\}
					F_X \text{ is non-decreasing}
				$$
			\end{subfigure}
		\end{figure}
	\end{defn}
	\begin{prop}
		If $F_X$ is the cumulative distribution function (\emph{cdf}) of a continuous random variable $X$, then $F_X$ is differentiable everywhere, except at certain subsets of points.
	\end{prop}
	Let us define
	$$ 
		f_X (t) = \frac{d}{dt} F_X (t) = \dot{F}_X (t)
	$$
	We can denote the following properties for $f_X$:
	\begin{enumerate}[$\quad\quad$(1)]
		\item$$f_X (t) \geq 0 \quad \quad \forall t$$
		\item \begin{align*}
			\int_{-\infty}^{+\infty} f_X (t) dt &= \lim_{\substack{a \to - \infty \\  b \to + \infty}} \int_a^b f_X (t)dt \\
			&= \lim_{\substack{a \to - \infty \\  b \to + \infty}} \int_a^b \dot{F}_X (t)dt \\
			&= \lim_{\substack{a \to - \infty \\  b \to + \infty}} \left( F_X (b) -  F_X (a) \right) \\
			&= 1-0 \\
			&= 1
		\end{align*}
	\end{enumerate}
\pagebreak
\subsection{Probability Density Function}
	\begin{defn}
		If $X$ is a continuous variable, then
		$$ f_X (t) = \dot{F}_X (t)$$
		is called the \textbf{probability density function} (pdf) of $X$.
	\end{defn}
	\begin{exmp}
		Set $f(t) = \frac{k}{1+t^2}$ whenever $t \in \R$. For what values of $k$ is $f(t)$ the pdf of a continuous random variable?
		\begin{sol}
			We must have $k \geq 0$, and also
			\begin{align*}
				\int_{-\infty}^{+\infty} \frac{k}{1+t^2} dt &= 1\\
				\therefore k \int_{-\infty}^{+\infty} \frac{1}{1+t^2} dt &= 1 \\
			\end{align*}
				$$
				\therefore f_X (\pi) = \underbrace{ \bigg(  \frac{1}{\pi} \bigg) }_{\substack{Normalizing \\ constant}}  \cdot \quad \bigg( \underbrace{ \frac{1}{1+t^2}}_{Kernel} \bigg) \quad \quad \forall \underbrace{t \in \R}_{Support}
				$$
		\end{sol}
		The above is the probability distribution function (\emph{pdf}) of the \textbf{Cauchy Distribution}. The cumulative distribution function (\emph{cdf}) of the Cauchy Distribution is
		\begin{align*}
			- F_X (t) &= \int_{- \infty}^{t} f_X (s) ds \\
			&= \frac{1}{\pi} \int_{-\infty}^{t} \frac{1}{1+s^2} ds \\
			&= \frac{1}{\pi} \arctan (t) + \frac{\pi}{2}
		\end{align*}
	\end{exmp}
\pagebreak
	\begin{exmp}
		The following plot represents the probability distribution function (\emph{pdf}) of a continuous random variable $X$. Find $P\big(-\frac12 \leq X \leq \frac14\big)$
		\begin{figure}[h]
			\begin{subfigure}{0.45\textwidth}
				\begin{center}
					\begin{tikzpicture}
						\draw [ultra thin, color=grey!30] (-3, -3) grid (3,3);
						\draw [->] (-3, -1) -- (3, -1) node[right]{$t$} ; 
						\draw [->] (0, -3) -- (0,3) node[above]{$f_X(t)$} ; 
						\draw (-0.1, 2) -- (0.1, 2) node[right]{1};
						\draw (-0.1, -1) -- (0.1, -1) node[left=2mm, below]{0};
						\draw (-2, -1.1) -- (-2, -0.9) node[ below = 2mm]{-1};
						\draw (2, -1.1) -- (2, -0.9) node[ below= 2mm]{1};
						\draw (-1, -1.1) -- (-1, -0.9) node[below = 2mm] {$-\frac{1}{2}$};
						\draw (0.5, -1.1) -- (0.5, -0.9) node[below = 2mm] {$\frac{1}{4}$};
						\draw [thick] plot [smooth, tension =0] coordinates {(-3, -1) (-2, -1) (0, 2) (2, -1) (3, -1)};
						\path [fill = blue!50, opacity =0.5] (-2,  -2) -- (2, -2) (0.5, 1.3) -- (0.5, -1) -- (-1, -1) -- (-1, 0.5) -- (0, 2);
					\end{tikzpicture}
				\end{center}
			\end{subfigure}
			\hfill
			\begin{subfigure}{0.45 \textwidth}
				We have
				\begin{align*}
					P \left( - \frac12 \leq X \leq \frac14 \right) &= P \left( X < \frac14 \right) - P \left(X < - \frac12\right) \\
					&= F_X \left( \frac14 \right) - F_X \left( - \frac12 \right) \\
					&= \int_{-\frac12}^{\frac14} f_X (t) dt \\
					&= 1 - \left( \frac18 + \frac{9}{32} \right) \\
					&= \frac{19}{32}
				\end{align*}
			\end{subfigure}
		\end{figure}
	\end{exmp}
	We will now list some properties for a continuous random variable $X$ with a \emph{cdf} $F_X (t)$ and  \emph{pdf} $f_X (t)$:
	\begin{enumerate}[$\quad \quad $ (1)]
		\item 
		\begin{align*}
			P (X \geq a) &= P (X > a) \\
			&= 1 - F_X (a) \\
			&= \int_0^{+\infty} f_X (t) dt
		\end{align*}
		\item 
		\begin{align*}
			P (X \leq a) &= P (X < a) \\
			&= F_X (a) \\
			&= \int_{- \infty}^a  f_X (t) dt
		\end{align*}
		\item 
		\begin{align*}
			P (a < X < b) &= P (a \leq X < b) \\
			&= P (a \leq X \leq b) \\
			&= P (a < X \leq b) \\
			&= \int_a^{+\infty} f_X (t) dt
		\end{align*}
	\end{enumerate}
\pagebreak
	\begin{prop}
		Let $X$ be a continuous random variable, and let $f_X$ be its \emph{pdf}. Let $\alpha, \beta \in \R$ such that $\alpha \neq 0$. We set $$ Y = \alpha X + \beta$$ 
		where $Y$ is a continuous random variable, and its \emph{pdf} is
		$$ f_Y (u) = \frac{1}{|\alpha|} f_X \left( \frac{u - \beta}{\alpha} \right)$$
	\end{prop}
	\begin{proof}
		For $\alpha >0$, the \emph{cdf} of $Y$ is
		\begin{align*}
			F_Y (u) &= P (Y \leq u) \\
			&= P (\alpha X + \beta \leq u) \\
			&= P \left( \alpha X \leq u + \beta  \right) \\
			&= P \left( X \leq \frac{u - \beta}{\alpha} \right)
		\end{align*}
		Therefore:
		$$ F_y(u) = F_X \left(  \frac{u-\beta}{\alpha}\right)$$
		Hence, the \emph{pdf} of $Y$ is
		\begin{align*}
			f_Y (u) &= \frac{d}{du} F_Y (u) \\ 
			&= \frac{1}{|\alpha|} f_X \left( \frac{u-\beta}{\alpha} \right)
		\end{align*}
	\end{proof}
	\begin{defn}
		Let $X$ be a continuous random variable and $f_X$ be the \emph{pdf}. Let $g: \R \to \R$ be a continuous function. Then, 
		$$ \E \big( g(X) \big) = \int_{-\infty}^{\infty} g(t) f_X (t) dt$$
	\end{defn}
	Here we have some  properties for a \emph{functional expected value} (\textbf{Definition 4.3}):
	\begin{enumerate}[$\quad\quad$(1)]
		\item If $X$ is a continuous random variable and $f: \R \to \R$ is a constant function (i.e. $f(t) = k \quad \forall t \in \R$), then:  $$ \E \big( f (t) \big) = k \quad \quad \forall t \in \R$$
		\item If $g: \R \to \R$ and  $h: \R \to \R$ are two continuous functions, then
		$$ \E \big( h (t) + g (t) \big) = \E \big( h (t) \big) + \E \big( g(t) \big)$$
	\end{enumerate}
	\begin{proof}
		\begin{enumerate}[$\quad\quad$(1)]
			\item
		\end{enumerate}
	\end{proof}
\end{document}      
